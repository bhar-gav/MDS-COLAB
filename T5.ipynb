{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN4cNVHEpDT31wYh8zACb6u"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"25f193345e0e4a848e5abdadc76a2bb0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3292da00d2544e30bc76191d2044143d","IPY_MODEL_80a8290340df408190903f34a6900cd8","IPY_MODEL_6aa16514afc24c0c8e297c39c74badc3"],"layout":"IPY_MODEL_e776470bfe4d4390a63c70bde4f856eb"}},"3292da00d2544e30bc76191d2044143d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8bfbfe445482450988bfe90c0fa4a99e","placeholder":"​","style":"IPY_MODEL_a4dcc2d5a2b64df3bdfa5277355c401f","value":"Map: 100%"}},"80a8290340df408190903f34a6900cd8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5664221f05e34cdea7be87ee4f3445da","max":547,"min":0,"orientation":"horizontal","style":"IPY_MODEL_17dc42100e44406d8c43407cc84629f1","value":547}},"6aa16514afc24c0c8e297c39c74badc3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e71d9fb5a2db4bea93746f79854094e7","placeholder":"​","style":"IPY_MODEL_afadf4955ac549a7b21d493c7e39d298","value":" 547/547 [00:44&lt;00:00, 12.44 examples/s]"}},"e776470bfe4d4390a63c70bde4f856eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8bfbfe445482450988bfe90c0fa4a99e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4dcc2d5a2b64df3bdfa5277355c401f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5664221f05e34cdea7be87ee4f3445da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17dc42100e44406d8c43407cc84629f1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e71d9fb5a2db4bea93746f79854094e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"afadf4955ac549a7b21d493c7e39d298":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"62075485a028477897487c8597a7d453":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e73bf9ac74b04550b8a032df61517f7a","IPY_MODEL_e352faa97278402da66b78a97d627000","IPY_MODEL_8afc9b91aa92420eb390b52357086f44"],"layout":"IPY_MODEL_bd073c3c119c4fc29222ba93b233c1d1"}},"e73bf9ac74b04550b8a032df61517f7a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3674941bd75146e984ceebfa039bdc2c","placeholder":"​","style":"IPY_MODEL_553ed739e3f84230b54979ad4623d34e","value":"Map: 100%"}},"e352faa97278402da66b78a97d627000":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6eefd41c1bab411290c074e6b01a8a5b","max":61,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4a22c6663f70472b8f6f970b5c239be0","value":61}},"8afc9b91aa92420eb390b52357086f44":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe9bd13cd8c04a399f598f84b9c25d6a","placeholder":"​","style":"IPY_MODEL_b3722d36a9eb44b1a9fdb3a3a8851997","value":" 61/61 [00:03&lt;00:00, 16.49 examples/s]"}},"bd073c3c119c4fc29222ba93b233c1d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3674941bd75146e984ceebfa039bdc2c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"553ed739e3f84230b54979ad4623d34e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6eefd41c1bab411290c074e6b01a8a5b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a22c6663f70472b8f6f970b5c239be0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fe9bd13cd8c04a399f598f84b9c25d6a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b3722d36a9eb44b1a9fdb3a3a8851997":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"96ee79a0937b4d0aa3d84896b8172bed":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8a6f2a5a37e64a0e8eb2a34bacbc3293","IPY_MODEL_b2d53a897e1449f0bdbd5e56f9a1cabd","IPY_MODEL_312d48abcd394031a27fcd97a1ea1330"],"layout":"IPY_MODEL_21d06e80b12f44dcaf01b0718bab6d5d"}},"8a6f2a5a37e64a0e8eb2a34bacbc3293":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b92ae0f2202c47cf82257a1bc90a47d5","placeholder":"​","style":"IPY_MODEL_d3b4b4c9363242dd844c389f1229fe8e","value":"config.json: 100%"}},"b2d53a897e1449f0bdbd5e56f9a1cabd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_56a323e5564040e981830a49abb1e4d0","max":1206,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b10c7a131c63462492c8a1fe4c03e613","value":1206}},"312d48abcd394031a27fcd97a1ea1330":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3b918f5eb98d49efa0a4db03ea15e618","placeholder":"​","style":"IPY_MODEL_790997be81de425489494bd8e2a1762e","value":" 1.21k/1.21k [00:00&lt;00:00, 65.5kB/s]"}},"21d06e80b12f44dcaf01b0718bab6d5d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b92ae0f2202c47cf82257a1bc90a47d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3b4b4c9363242dd844c389f1229fe8e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"56a323e5564040e981830a49abb1e4d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b10c7a131c63462492c8a1fe4c03e613":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3b918f5eb98d49efa0a4db03ea15e618":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"790997be81de425489494bd8e2a1762e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8a1c76ad55ee496092c45c653799da05":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c38849b2711c46c4bb795b775a2fdc60","IPY_MODEL_335c8c0601da458d817a188e6173e72a","IPY_MODEL_e90ec56afe15445ba47890c2251072bb"],"layout":"IPY_MODEL_500829b8c4b0457f867fc1906f75ed2d"}},"c38849b2711c46c4bb795b775a2fdc60":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b21194b5df2441d790692cb5b6d035bf","placeholder":"​","style":"IPY_MODEL_21f4148afc3c4cd3becc4a76230aabc3","value":"model.safetensors: 100%"}},"335c8c0601da458d817a188e6173e72a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca6f92bb45584608994ab0da1eb3fd35","max":242043056,"min":0,"orientation":"horizontal","style":"IPY_MODEL_111cea8adf614704b15d705e0c211a18","value":242043056}},"e90ec56afe15445ba47890c2251072bb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4c8e79853424b4fb77d299f386402d2","placeholder":"​","style":"IPY_MODEL_f5bd613cb81640438bc67cf8211ff964","value":" 242M/242M [00:02&lt;00:00, 181MB/s]"}},"500829b8c4b0457f867fc1906f75ed2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b21194b5df2441d790692cb5b6d035bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"21f4148afc3c4cd3becc4a76230aabc3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ca6f92bb45584608994ab0da1eb3fd35":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"111cea8adf614704b15d705e0c211a18":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e4c8e79853424b4fb77d299f386402d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5bd613cb81640438bc67cf8211ff964":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f1f6ef7772ff493b82d46dc2aafcf567":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9ecf48f9ca61460e871d686ec4823aa9","IPY_MODEL_38b0eeb5c76541ae964ac8c0195411fd","IPY_MODEL_ae7e8687564540f6acf3ed244925b6a1"],"layout":"IPY_MODEL_6d53231a720c4cf89d106b4a281493ad"}},"9ecf48f9ca61460e871d686ec4823aa9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7bc801117b8b4e8da69600e4c191d4cc","placeholder":"​","style":"IPY_MODEL_fbe1264d226a4b6a87f302b85f590ec5","value":"generation_config.json: 100%"}},"38b0eeb5c76541ae964ac8c0195411fd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7ab5a0a3bb04687ba7086e9ed50f0c3","max":147,"min":0,"orientation":"horizontal","style":"IPY_MODEL_06e4316de22f4d61aa94ba8c547ec58a","value":147}},"ae7e8687564540f6acf3ed244925b6a1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5982cbd7a5274aa782d938e4ac9415b3","placeholder":"​","style":"IPY_MODEL_aa3d9a8f759a42058b6e592cd259a87b","value":" 147/147 [00:00&lt;00:00, 5.20kB/s]"}},"6d53231a720c4cf89d106b4a281493ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7bc801117b8b4e8da69600e4c191d4cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fbe1264d226a4b6a87f302b85f590ec5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c7ab5a0a3bb04687ba7086e9ed50f0c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"06e4316de22f4d61aa94ba8c547ec58a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5982cbd7a5274aa782d938e4ac9415b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa3d9a8f759a42058b6e592cd259a87b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1d0809a4c2f445b8ada2a1dffaaf616f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b2073f0d379449f3aa24e6b59faf8db8","IPY_MODEL_5007ff5acade4ec09e1650de08aa593a","IPY_MODEL_52ceb771a418431797b002ff6b295024"],"layout":"IPY_MODEL_8c8d08ee4649458aac8d7cb3d6230582"}},"b2073f0d379449f3aa24e6b59faf8db8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d3bddc80229419dacb67437f25ff956","placeholder":"​","style":"IPY_MODEL_fc9182a1c8464209b6de8c71e158dd8c","value":"Downloading builder script: 100%"}},"5007ff5acade4ec09e1650de08aa593a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0cc57923aecc452eb3fa1de9439fa735","max":6270,"min":0,"orientation":"horizontal","style":"IPY_MODEL_23c35b826a2640b3af04f6fcc5885383","value":6270}},"52ceb771a418431797b002ff6b295024":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_124027340c4d4fd2bc843f5b957948d2","placeholder":"​","style":"IPY_MODEL_edd506273199497881a9c8b245a71d08","value":" 6.27k/6.27k [00:00&lt;00:00, 96.7kB/s]"}},"8c8d08ee4649458aac8d7cb3d6230582":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d3bddc80229419dacb67437f25ff956":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc9182a1c8464209b6de8c71e158dd8c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0cc57923aecc452eb3fa1de9439fa735":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23c35b826a2640b3af04f6fcc5885383":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"124027340c4d4fd2bc843f5b957948d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"edd506273199497881a9c8b245a71d08":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":7,"metadata":{"id":"CsxYR8zNYnnt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734014669723,"user_tz":-330,"elapsed":3726,"user":{"displayName":"21dcs022 BHARGAV","userId":"14054464433858683476"}},"outputId":"2b0ffd27-f2e6-4191-fd42-499c20c6fd51"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content\n","/content/drive/My Drive/Colab Notebooks/MDS\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","print(os.getcwd())\n","folder = \"/content/drive/My Drive/Colab Notebooks/MDS\"\n","os.chdir(folder)\n","print(os.getcwd())"]},{"cell_type":"markdown","source":["\n","\n","---\n","preprocessing already applied :\n","\n","```\n","# This code is already implemented on file\n","```\n","\n","\n"],"metadata":{"id":"GK2vsbilTTTK"}},{"cell_type":"markdown","source":["**Dataset Info**"],"metadata":{"id":"rkrJtuvabfrf"}},{"cell_type":"code","source":["import pandas as pd\n","df=pd.read_csv(\"dataset/combined_data_resolved.csv\")\n","df.head()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"collapsed":true,"id":"7EF33q9huGZr","executionInfo":{"status":"ok","timestamp":1731093243232,"user_tz":-330,"elapsed":3439,"user":{"displayName":"21dcs022 BHARGAV","userId":"14054464433858683476"}},"outputId":"9bb46038-4bd3-4821-8c87-1b7e94f3f659"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   Index                                            Article  \\\n","0      1  \\nD-Tree Grammars\\n\\ndesigned to share some of...   \n","1      2  \\nJoint Learning Improves Semantic Role Labeli...   \n","2      3  \\nBilingually-Constrained (Monolingual) Shift-...   \n","3      4  \\nA Generative Constituent-Context Model For I...   \n","4      5  \\nWord Association Norms Mutual Information An...   \n","\n","                                             Summary  \n","0  Title: D-Tree Grammars\\n\\nAbstract: designed t...  \n","1  Title: Joint Learning Improves Semantic Role L...  \n","2  Title: Bilingually-Constrained (Monolingual) S...  \n","3  Title: A Generative Constituent-Context Model ...  \n","4  Title: Word Association Norms Mutual Informati...  "],"text/html":["\n","  <div id=\"df-b7d43d3c-3c01-44f0-974b-efa847bc8085\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Index</th>\n","      <th>Article</th>\n","      <th>Summary</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>\\nD-Tree Grammars\\n\\ndesigned to share some of...</td>\n","      <td>Title: D-Tree Grammars\\n\\nAbstract: designed t...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>\\nJoint Learning Improves Semantic Role Labeli...</td>\n","      <td>Title: Joint Learning Improves Semantic Role L...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>\\nBilingually-Constrained (Monolingual) Shift-...</td>\n","      <td>Title: Bilingually-Constrained (Monolingual) S...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>\\nA Generative Constituent-Context Model For I...</td>\n","      <td>Title: A Generative Constituent-Context Model ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>\\nWord Association Norms Mutual Information An...</td>\n","      <td>Title: Word Association Norms Mutual Informati...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b7d43d3c-3c01-44f0-974b-efa847bc8085')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-b7d43d3c-3c01-44f0-974b-efa847bc8085 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-b7d43d3c-3c01-44f0-974b-efa847bc8085');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-05277646-3fa6-45c7-b1de-982e62980fdf\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-05277646-3fa6-45c7-b1de-982e62980fdf')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-05277646-3fa6-45c7-b1de-982e62980fdf button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 608,\n  \"fields\": [\n    {\n      \"column\": \"Index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 175,\n        \"min\": 1,\n        \"max\": 608,\n        \"num_unique_values\": 608,\n        \"samples\": [\n          110,\n          11,\n          185\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Article\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 608,\n        \"samples\": [\n          \"\\nCoarse-To-Fine N-Best Parsing And MaxEnt Discriminative Reranking\\n\\nDiscriminative reranking is one method for constructing high-performance statistical parsers (Collins, 2000).\\nA discriminative reranker requires a source of candidate parses for each sentence.\\nThis paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse-to-fine generative parser (Charniak, 2000).\\nThis method generates 50-best lists that are of substantially higher quality than previously obtainable.\\nWe used these parses as the input to a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002) that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91.0% on sentences of length 100 or less.\\n\\nWe describe a reranking parser which uses a regularized MaxEnt reranker to select the best parse from the 50-best parses returned by a generative parsing model.\\nThe 50-best parser is a probabilistic parser that on its own produces high quality parses; the maximum probability parse trees (according to the parser\\u2019s model) have an f-score of 0.897 on section 23 of the Penn Treebank (Charniak, 2000), which is still state-of-the-art.\\nHowever, the 50 best (i.e., the 50 highest probability) parses of a sentence often contain considerably better parses (in terms of f-score); this paper describes a 50-best parsing algorithm with an oracle f-score of 96.8 on the same data.\\nThe reranker attempts to select the best parse for a sentence from the 50-best list of possible parses for the sentence.\\nBecause the reranker only has to consider a relatively small number of parses per sentences, it is not necessary to use dynamic programming, which permits the features to be essentially arbitrary functions of the parse trees.\\nWhile our reranker does not achieve anything like the oracle f-score, the parses it selects do have an f-score of 91.0, which is considerably better than the maximum probability parses of the n-best parser.\\nIn more detail, for each string s the n-best parsing algorithm described in section 2 returns the n highest probability parses Y(s) = {y1(s), ... , yn(s)} together with the probability p(y) of each parse y according to the parser\\u2019s probability model.\\nThe number n of parses was set to 50 for the experiments described here, but some simple sentences actually received fewer than 50 parses (so n is actually a function of s).\\nEach yield or terminal string in the training, development and test data sets is mapped to such an n-best list of parse/probability pairs; the cross-validation scheme described in Collins (2000) was used to avoid training the n-best parser on the sentence it was being used to parse.\\nA feature extractor, described in section 3, is a vector of m functions f = (fl, ... , fm), where each fj maps a parse y to a real number fj(y), which is the value of the jth feature on y.\\nSo a feature extractor maps each y to a vector of feature values f(y) = (f1(y), ..., fm(y)).\\nOur reranking parser associates a parse with a score v\\u03b8(y), which is a linear function of the feature values f(y).\\nThat is, each feature fj is associated with a weight \\u03b8j, and the feature values and weights define the score v\\u03b8(y) of each parse y as follows: Given a string s, the reranking parser\\u2019s output \\u02c6y(s) on string s is the highest scoring parse in the n-best parses Y(s) for s, i.e., The feature weight vector \\u03b8 is estimated from the labelled training corpus as described in section 4.\\nBecause we use labelled training data we know the correct parse y?\\n(s) for each sentence s in the training data.\\nThe correct parse y?\\n(s) is not always a member of the n-best parser\\u2019s output Y(s), but we can identify the parses Y+(s) in Y(s) with the highest f-scores.\\nInformally, the estimation procedure finds a weight vector \\u03b8 that maximizes the score v\\u03b8(y) of the parses y E Y+(s) relative to the scores of the other parses in Y(s), for each s in the training data.\\n\\nThe major difficulty in n-best parsing, compared to 1-best parsing, is dynamic programming.\\nFor example, n-best parsing is straight-forward in best-first search or beam search approaches that do not use dynamic programming: to generate more than one parse, one simply allows the search mechanism to create successive versions to one\\u2019s heart\\u2019s content.\\nA good example of this is the Roark parser (Roark, 2001) which works left-to right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning.\\nAt the end one has a beam-width\\u2019s number of best parses (Roark, 2001).\\nThe Collins parser (Collins, 1997) does use dynamic programming in its search.\\nThat is, whenever a constituent with the same history is generated a second time, it is discarded if its probability is lower than the original version.\\nIf the opposite is true, then the original is discarded.\\nThis is fine if one only wants the first-best, but obviously it does not directly enumerate the n-best parses.\\nHowever, Collins (Collins, 2000; Collins and Koo, in submission) has created an nbest version of his parser by turning off dynamic programming (see the user\\u2019s guide to Bikel\\u2019s re-implementation of Collins\\u2019 parser, http://www.cis.upenn.edu/ dbikel/software.html#statparser).\\nAs with Roark\\u2019s parser, it is necessary to add a beam-width constraint to make the search tractable.\\nWith a beam width of 1000 the parser returns something like a 50-best list (Collins, personal communication), but the actual number of parses returned for each sentences varies.\\nHowever, turning off dynamic programming results in a loss in efficiency.\\nIndeed, Collins\\u2019s n-best list of parses for section 24 of the Penn tree-bank has some sentences with only a single parse, because the n-best parser could not find any parses.\\nNow there are two known ways to produce n-best parses while retaining the use of dynamic programming: the obvious way and the clever way.\\nThe clever way is based upon an algorithm developed by Schwartz and Chow (1990).\\nRecall the key insight in the Viterbi algorithm: in the optimal parse the parsing decisions at each of the choice points that determine a parse must be optimal, since otherwise one could find a better parse.\\nThis insight extends to n-best parsing as follows.\\nConsider the secondbest parse: if it is to differ from the best parse, then at least one of its parsing decisions must be suboptimal.\\nIn fact, all but one of the parsing decisions in second-best parse must be optimal, and the one suboptimal decision must be the second-best choice at that choice point.\\nFurther, the nth-best parse can only involve at most n suboptimal parsing decisions, and all but one of these must be involved in one of the second through the n\\u22121th-best parses.\\nThus the basic idea behind this approach to n-best parsing is to first find the best parse, then find the second-best parse, then the third-best, and so on.\\nThe algorithm was originally described for hidden Markov models.\\nSince this first draft of this paper we have become aware of two PCFG implementations of this algorithm (Jimenez and Marzal, 2000; Huang and Chang, 2005).\\nThe first was tried on relatively small grammars, while the second was implemented on top of the Bikel re-implementation of the Collins v\\u03b8(y). parser (Bikel, 2004) and achieved oracle results for 50-best parses similar to those we report below.\\nHere, however, we describe how to find n-best parses in a more straight-forward fashion.\\nRather than storing a single best parse of each edge, one stores n of them.\\nThat is, when using dynamic programming, rather than throwing away a candidate if it scores less than the best, one keeps it if it is one of the top n analyses for this edge discovered so far.\\nThis is really very straight-forward.\\nThe problem is space.\\nDynamic programming parsing algorithms for PCFGs require O(m2) dynamic programming states, where m is the length of the sentence, so an n-best parsing algorithm requires O(nm2).\\nHowever things get much worse when the grammar is bilexicalized.\\nAs shown by Eisner (Eisner and Satta, 1999) the dynamic programming algorithms for bilexicalized PCFGs require O(m3) states, so a n-best parser would require O(nm3) states.\\nThings become worse still in a parser like the one described in Charniak (2000) because it conditions on (and hence splits the dynamic programming states according to) features of the grandparent node in addition to the parent, thus multiplying the number of possible dynamic programming states even more.\\nThus nobody has implemented this version.\\nThere is, however, one particular feature of the Charniak parser that mitigates the space problem: it is a \\u201ccoarse-to-fine\\u201d parser.\\nBy \\u201ccoarse-to-fine\\u201d we mean that it first produces a crude version of the parse using coarse-grained dynamic programming states, and then builds fine-grained analyses by splitting the most promising of coarse-grained states.\\nA prime example of this idea is from Goodman (1997), who describes amethod for producing a simple but crude approximate grammar of a standard context-free grammar.\\nHe parses a sentence using the approximate grammar, and the results are used to constrain the search for a parse with the full CFG.\\nHe finds that total parsing time is greatly reduced.\\nA somewhat different take on this paradigm is seen in the parser we use in this paper.\\nHere the parser first creates a parse forest based upon a much less complex version of the complete grammar.\\nIn particular, it only looks at standard CFG features, the parent and neighbor labels.\\nBecause this grammar encodes relatively little state information, its dynamic programming states are relatively coarse and hence there are comparatively few of them, so it can be efficiently parsed using a standard dynamic programming bottom-up CFG parser.\\nHowever, precisely because this first stage uses a grammar that ignores many important contextual features, the best parse it finds will not, in general, be the best parse according to the finer-grained second-stage grammar, so clearly we do not want to perform best-first parsing with this grammar.\\nInstead, the output of the first stage is a polynomial-sized packed parse forest which records the left and right string positions for each local tree in the parses generated by this grammar.\\nThe edges in the packed parse forest are then pruned, to focus attention on the coarsegrained states that are likely to correspond to highprobability fine-grained states.\\nThe edges are then pruned according to their marginal probability conditioned on the string s being parsed as follows: Here nij,k is a constituent of type i spanning the words from j to k, \\u03b1(nij,k) is the outside probability of this constituent, and Q(nij,k) is its inside probability.\\nFrom parse forest both \\u03b1 and Q can be computed in time proportional to the size of the compact forest.\\nThe parser then removes all constituents nij,k whose probability falls below some preset threshold.\\nIn the version of this parser available on the web, this threshold is on the order of 10\\u22124.\\nThe unpruned edges are then exhaustively evaluated according to the fine-grained probabilistic model; in effect, each coarse-grained dynamic programming state is split into one or more fine-grained dynamic programming states.\\nAs noted above, the fine-grained model conditions on information that is not available in the coarse-grained model.\\nThis includes the lexical head of one\\u2019s parents, the part of speech of this head, the parent\\u2019s and grandparent\\u2019s category labels, etc.\\nThe fine-grained states investigated by the parser are constrained to be refinements of the coarse-grained states, which drastically reduces the number of fine-grained states that need to be investigated.\\nIt is certainly possible to do dynamic programming parsing directly with the fine-grained grammar, but precisely because the fine-grained grammar conditions on a wide variety of non-local contextual information there would be a very large number of different dynamic programming states, so direct dynamic programming parsing with the fine-grained grammar would be very expensive in terms of time and memory.\\nAs the second stage parse evaluates all the remaining constituents in all of the contexts in which they appear (e.g., what are the possible grand-parent labels) it keeps track of the most probable expansion of the constituent in that context, and at the end is able to start at the root and piece together the overall best parse.\\nNow comes the easy part.\\nTo create a 50-best parser we simply change the fine-grained version of 1-best algorithm in accordance with the \\u201cobvious\\u201d scheme outlined earlier in this section.\\nThe first, coarse-grained, pass is not changed, but the second, fine-grained, pass keeps the n-best possibilities at each dynamic programming state, rather than keeping just first best.\\nWhen combining two constituents to form a larger constituent, we keep the best 50 of the 2500 possibilities they offer.\\nNaturally, if we keep each 50-best list sorted, we do nothing like 2500 operations.\\nThe experimental question is whether, in practice, the coarse-to-fine architecture keeps the number of dynamic programming states sufficiently low that space considerations do not defeat us.\\nThe answer seems to be yes.\\nWe ran the algorithm on section 24 of the Penn WSJ tree-bank using the default pruning settings mentioned above.\\nTable 1 shows how the number of fine-grained dynamic programming states increases as a function of sentence length for the sentences in section 24 of the Treebank.\\nThere are no sentences of length greater than 69 in this section.\\nColumns two to four show the number of sentences in each bucket, their average length, and the average number of fine-grained dynamic programming structures per sentence.\\nThe final column gives the value of the function 100*L1.5 where L is the average length of sentences in the bucket.\\nExcept for bucket 6, which is abnormally low, it seems that this add-hoc function tracks the number of structures quite well.\\nThus the number of dynamic programming states does not grow as L2, much less as L3.\\nTo put the number of these structures per sentence in perspective, consider the size of such structures.\\nEach one must contain a probability, the nonterminal label of the structure, and a vector of pointers to it\\u2019s children (an average parent has slightly more than two children).\\nIf one were concerned about every byte this could be made quite small.\\nIn our implementation probably the biggest factor is the STL overhead on vectors.\\nIf we figure we are using, say, 25 bytes per structure, the total space required is only 1.25Mb even for 50,000 dynamic programming states, so it is clearly not worth worrying about the memory required.\\nThe resulting n-bests are quite good, as shown in Table 2.\\n(The results are for all sentences of section 23 of the WSJ tree-bank of length \\u2264 100.)\\nFrom the 1-best result we see that the base accuracy of the parser is 89.7%.1 2-best and 10-best show dramatic oracle-rate improvements.\\nAfter that things start to slow down, and we achieve an oracle rate of 0.968 at 50-best.\\nTo put this in perspective, Roark (Roark, 2001) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses.\\nFor the case cited his parser returns, on average, 70 parses per sentence.\\nFinally, we note that 50-best parsing is only a factor of two or three slower than 1-best.\\n\\nThis section describes how each parse y is mapped to a feature vector f(y) = (f1(y), ... , fm(y)).\\nEach feature fj is a function that maps a parse to a real number.\\nThe first feature f1(y) = log p(y) is the logarithm of the parse probability p according to the n-best parser model.\\nThe other features are integer valued; informally, each feature is associated with a particular configuration, and the feature\\u2019s value fj(y) is the number of times that the configuration that fj indicates.\\nFor example, the feature feat pizza(y) counts the number of times that a phrase in y headed by eat has a complement phrase headed by pizza.\\nFeatures belong to feature schema, which are abstract schema from which specific features are instantiated.\\nFor example, the feature feat pizza is an instance of the \\u201cHeads\\u201d schema.\\nFeature schema are often parameterized in various ways.\\nFor example, the \\u201cHeads\\u201d schema is parameterized by the type of heads that the feature schema identifies.\\nFollowing Grimshaw (1997), we associate each phrase with a lexical head and a function head.\\nFor example, the lexical head of an NP is a noun while the functional head of an NP is a determiner, and the lexical head of a VP is a main verb while the functional head of VP is an auxiliary verb.\\nWe experimented with various kinds of feature selection, and found that a simple count threshold performs as well as any of the methods we tried.\\nSpecifically, we ignored all features that did not vary on the parses of at least t sentences, where t is the count threshold.\\nIn the experiments described below t = 5, though we also experimented with t = 2.\\nThe rest of this section outlines the feature schemata used in the experiments below.\\nThese feature schemata used here were developed using the n-best parses provided to us by Michael Collins approximately a year before the n-best parser described here was developed.\\nWe used the division into preliminary training and preliminary development data sets described in Collins (2000) while experimenting with feature schemata; i.e., the first 36,000 sentences of sections 2\\u201320 were used as preliminary training data, and the remaining sentences of sections 20 and 21 were used as preliminary development data.\\nIt is worth noting that developing feature schemata is much more of an art than a science, as adding or deleting a single schema usually does not have a significant effect on performance, yet the overall impact of many well-chosen schemata can be dramatic.\\nUsing the 50-best parser output described here, there are 1,148,697 features that meet the count threshold of at least 5 on the main training data (i.e., Penn treebank sections 2\\u201321).\\nWe list each feature schema\\u2019s name, followed by the number of features in that schema with a count of at least 5, together with a brief description of the instances of the schema and the schema\\u2019s parameters.\\nCoPar (10) The instances of this schema indicate conjunct parallelism at various different depths.\\nFor example, conjuncts which have the same label are parallel at depth 0, conjuncts with the same label and whose children have the same label are parallel at depth 1, etc.\\nCoLenPar (22) The instances of this schema indicate the binned difference in length (in terms of number of preterminals dominated) in adjacent conjuncts in the same coordinated structures, conjoined with a boolean flag that indicates whether the pair is final in the coordinated phrase.\\nRightBranch (2) This schema enables the reranker to prefer right-branching trees.\\nOne instance of this schema returns the number of nonterminal nodes that lie on the path from the root node to the right-most non-punctuation preterminal node, and the other instance of this schema counts the number of the other nonterminal nodes in the parse tree.\\nHeavy (1049) This schema classifies nodes by their category, their binned length (i.e., the number of preterminals they dominate), whether they are at the end of the sentence and whether they are followed by punctuation.\\nNeighbours (38,245) This schema classifies nodes by their category, their binned length, and the part of speech categories of the E1 preterminals to the node\\u2019s left and the $2 preterminals to the node\\u2019s right.\\nE1 and E2 are parameters of this schema; here E1 = 1 or E1 = 2 and E2 = 1.\\nRule (271,655) The instances of this schema are local trees, annotated with varying amounts of contextual information controlled by the schema\\u2019s parameters.\\nThis schema was inspired by a similar schema in Collins and Koo (in submission).\\nThe parameters to this schema control whether nodes are annotated with their preterminal heads, their terminal heads and their ancestors\\u2019 categories.\\nAn additional parameter controls whether the feature is specialized to embedded or non-embedded clauses, which roughly corresponds to Emonds\\u2019 \\u201cnonroot\\u201d and \\u201croot\\u201d contexts (Emonds, 1976).\\nNGram (54,567) The instances of this schema are E-tuples of adjacent children nodes of the same parent.\\nThis schema was inspired by a similar schema in Collins and Koo (in submission).\\nThis schema has the same parameters as the Rule schema, plus the length E of the tuples of children (E = 2 here).\\nHeads (208,599) The instances of this schema are tuples of head-to-head dependencies, as mentioned above.\\nThe category of the node that is the least common ancestor of the head and the dependent is included in the instance (this provides a crude distinction between different classes of arguments).\\nThe parameters of this schema are whether the heads involved are lexical or functional heads, the number of heads in an instance, and whether the lexical item or just the head\\u2019s part of speech are included in the instance.\\nLezFunHeads (2,299) The instances of this feature are the pairs of parts of speech of the lexical head and the functional head of nodes in parse trees.\\nWProj (158,771) The instances of this schema are preterminals together with the categories of E of their closest maximal projection ancestors.\\nThe parameters of this schema control the number E of maximal projections, and whether the preterminals and the ancestors are lexicalized.\\nWord (49,097) The instances of this schema are lexical items together with the categories of E of their immediate ancestor nodes, where E is a schema parameter (E = 2 or E = 3 here).\\nThis feature was inspired by a similar feature in Klein and Manning (2003).\\nHeadTree (72,171) The instances of this schema are tree fragments consisting of the local trees consisting of the projections of a preterminal node and the siblings of such projections.\\nThis schema is parameterized by the head type (lexical or functional) used to determine the projections of a preterminal, and whether the head preterminal is lexicalized.\\nNGramTree (291,909) The instances of this schema are subtrees rooted in the least common ancestor of E contiguous preterminal nodes.\\nThis schema is parameterized by the number E of contiguous preterminals (E = 2 or E = 3 here) and whether these preterminals are lexicalized.\\n\\nThis section explains how we estimate the feature weights 0 = (01, ... , 0m) for the feature functions f = (f1, ... , fm).\\nWe use a MaxEnt estimator to find the feature weights \\u02c60, where L is the loss function and R is a regularization penalty term: The training data D = (s1, ... , sn,) is a sequence of sentences and their correct parses y?\\n(s1), ... , y?(sn).\\nWe used the 20-fold crossvalidation technique described in Collins (2000) to compute the n-best parses Y(s) for each sentence s in D. In general the correct parse y?\\n(s) is not a member of Y(s), so instead we train the reranker to identify one of the best parses Y+(s) = arg maxy\\u2208Y(s) Fy,,(s)(y) in the n-best parser\\u2019s output, where Fy,,(y) is the Parseval f-score of y evaluated with respect to y?.\\nBecause there may not be a unique best parse for each sentence (i.e., |Y+(s) |> 1 for some sentences s) we used the variant of MaxEnt described in Riezler et al. (2002) for partially labelled training data.\\nRecall the standard MaxEnt conditional probability model for a parse y E Y: The loss function LD proposed in Riezler et al. (2002) is just the negative log conditional likelihood of the best parses Y+(s) relative to the n-best parser output Y(s): The partial derivatives of this loss function, which are required by the numerical estimation procedure, are: In the experiments reported here, we used a Gaussian or quadratic regularizer R(w) = cPmj=1 w2j, where c is an adjustable parameter that controls the amount of regularization, chosen to optimize the reranker\\u2019s f-score on the development set (section 24 of the treebank).\\nWe used the Limited Memory Variable Metric optimization algorithm from the PETSc/TAO optimization toolkit (Benson et al., 2004) to find the optimal feature weights \\u03b8\\u02c6 because this method seems substantially faster than comparable methods (Malouf, 2002).\\nThe PETSc/TAO toolkit provides a variety of other optimization algorithms and flags for controlling convergence, but preliminary experiments on the Collins\\u2019 trees with different algorithms and early stopping did not show any performance improvements, so we used the default PETSc/TAO setting for our experiments here.\\n\\nWe evaluated the performance of our reranking parser using the standard PARSEVAL metrics.\\nWe n-best trees f-score New 0.9102 Collins 0.9037 best trees, with weights estimated from sections 2\\u2013 21 and the regularizer constant c adjusted for optimal f-score on section 24 and evaluated on sentences of length less than 100 in section 23. trained the n-best parser on sections 2\\u201321 of the Penn Treebank, and used section 24 as development data to tune the mixing parameters of the smoothing model.\\nSimilarly, we trained the feature weights \\u03b8 with the MaxEnt reranker on sections 2\\u201321, and adjusted the regularizer constant c to maximize the f-score on section 24 of the treebank.\\nWe did this both on the trees supplied to us by Michael Collins, and on the output of the n-best parser described in this paper.\\nThe results are presented in Table 3.\\nThe n-best parser\\u2019s most probable parses are already of state-of-the-art quality, but the reranker further improves the f-score.\\n\\nThis paper has described a dynamic programming n-best parsing algorithm that utilizes a heuristic coarse-to-fine refinement of parses.\\nBecause the coarse-to-fine approach prunes the set of possible parse edges beforehand, a simple approach which enumerates the n-best analyses of each parse edge is not only practical but quite efficient.\\nWe use the 50-best parses produced by this algorithm as input to a MaxEnt discriminative reranker.\\nThe reranker selects the best parse from this set of parses using a wide variety of features.\\nThe system we described here has an f-score of 0.91 when trained and tested using the standard PARSEVAL framework.\\nThis result is only slightly higher than the highest reported result for this test-set, Bod\\u2019s (.907) (Bod, 2003).\\nMore to the point, however, is that the system we describe is reasonably efficient so it can be used for the kind of routine parsing currently being handled by the Charniak or Collins parsers.\\nA 91.0 f-score represents a 13% reduction in fmeasure error over the best of these parsers.2 Both the 50-best parser, and the reranking parser can be found at ftp://ftp.cs.brown.edu/pub/nlparser/, named parser and reranker respectively.\\nAcknowledgements We would like to thanks Michael Collins for the use of his data and many helpful comments, and Liang Huang for providing an early draft of his paper and very useful comments on our paper.\\nFinally thanks to the National Science Foundation for its support (NSF IIS-0112432, NSF 9721276, and NSF DMS-0074276).\\n\",\n          \"\\nScaling Web-Based Acquisition Of Entailment Relations\\n\\nParaphrase recognition is a critical step for natural language interpretation.\\nAccordingly, many NLP applications would benefit from high coverage knowledge bases of paraphrases.\\nHowever, the scalability of state-of-the-art paraphrase acquisition approaches is still limited.\\nWe present a fully unsupervised learning algorithm for Web-based extraction an extended model of paraphrases.\\nWe focus on increased scalability and generality with respect to prior work, eventually aiming at a full scale knowledge base.\\nOur current implementation of the algorithm takes as its input a verb lexicon and for each verb searches the Web for related syntactic entailment templates.\\nExperiments show promising results with respect to the ultimate goal, achieving much better scalability than prior Web-based methods.\\n\\nModeling semantic variability in language has drawn a lot of attention in recent years.\\nMany applications like QA, IR, IE and Machine Translation (Moldovan and Rus, 2001; Hermjakob et al., 2003; Jacquemin, 1999) have to recognize that the same meaning can be expressed in the text in a huge variety of surface forms.\\nSubstantial research has been dedicated to acquiring paraphrase patterns, which represent various forms in which a certain meaning can be expressed.\\nFollowing (Dagan and Glickman, 2004) we observe that a somewhat more general notion needed for applications is that of entailment relations (e.g.\\n(Moldovan and Rus, 2001)).\\nThese are directional relations between two expressions, where the meaning of one can be entailed from the meaning of the other.\\nFor example \\u201cX acquired Y\\u201d entails \\u201cX owns Y\\u201d.\\nThese relations provide a broad framework for representing and recognizing semantic variability, as proposed in (Dagan and Glickman, 2004).\\nFor example, if a QA system has to answer the question \\u201cWho owns Overture?\\u201d and the corpus includes the phrase \\u201cYahoo acquired Overture\\u201d, the system can use the known entailment relation to conclude that this phrase really indicates the desired answer.\\nMore examples of entailment relations, acquired by our method, can be found in Table 1 (section 4).\\nTo perform such inferences at a broad scale, applications need to possess a large knowledge base (KB) of entailment patterns.\\nWe estimate such a KB should contain from between a handful to a few dozens of relations per meaning, which may sum to a few hundred thousands of relations for a broad domain, given that a typical lexicon includes tens of thousands of words.\\nOur research goal is to approach unsupervised acquisition of such a full scale KB.\\nWe focus on developing methods that acquire entailment relations from the Web, the largest available resource.\\nTo this end substantial improvements are needed in order to promote scalability relative to current Webbased approaches.\\nIn particular, we address two major goals: reducing dramatically the complexity of required auxiliary inputs, thus enabling to apply the methods at larger scales, and generalizing the types of structures that can be acquired.\\nThe algorithms described in this paper were applied for acquiring entailment relations for verb-based expressions.\\nThey successfully discovered several relations on average per each randomly selected expression.\\n\\nThis section provides a qualitative view of prior work, emphasizing the perspective of aiming at a full-scale paraphrase resource.\\nAs there are still no standard benchmarks, current quantitative results are not comparable in a consistent way.\\nThe major idea in paraphrase acquisition is often to find linguistic structures, here termed templates, that share the same anchors.\\nAnchors are lexical elements describing the context of a sentence.\\nTemplates that are extracted from different sentences and connect the same anchors in these sentences, are assumed to paraphrase each other.\\nFor example, the sentences \\u201cYahoo bought Overture\\u201d and \\u201cYahoo acquired Overture\\u201d share the anchors {X=Yahoo, Y=Overture}, suggesting that the templates \\u2018X buy Y\\u2019 and \\u2018X acquire Y\\u2019 paraphrase each other.\\nAlgorithms for paraphrase acquisition address two problems: (a) finding matching anchors and (b) identifying template structure, as reviewed in the next two subsections.\\nThe prominent approach for paraphrase learning searches sentences that share common sets of multiple anchors, assuming they describe roughly the same fact or event.\\nTo facilitate finding many matching sentences, highly redundant comparable corpora have been used.\\nThese include multiple translations of the same text (Barzilay and McKeown, 2001) and corresponding articles from multiple news sources (Shinyama et al., 2002; Pang et al., 2003; Barzilay and Lee, 2003).\\nWhile facilitating accuracy, we assume that comparable corpora cannot be a sole resource due to their limited availability.\\nAvoiding a comparable corpus, (Glickman and Dagan, 2003) developed statistical methods that match verb paraphrases within a regular corpus.\\nTheir limited scale results, obtaining several hundred verb paraphrases from a 15 million word corpus, suggest that much larger corpora are required.\\nNaturally, the largest available corpus is the Web.\\nSince exhaustive processing of the Web is not feasible, (Duclaye et al., 2002) and (Ravichandran and Hovy, 2002) attempted bootstrapping approaches, which resemble the mutual bootstrapping method for Information Extraction of (Riloff and Jones, 1999).\\nThese methods start with a provided known set of anchors for a target meaning.\\nFor example, the known anchor set {Mozart, 1756} is given as input in order to find paraphrases for the template \\u2018X born in Y\\u2019.\\nWeb searching is then used to find occurrences of the input anchor set, resulting in new templates that are supposed to specify the same relation as the original one (\\u201cborn in\\u201d).\\nThese new templates are then exploited to get new anchor sets, which are subsequently processed as the initial {Mozart, 1756}.\\nEventually, the overall procedure results in an iterative process able to induce templates from anchor sets and vice versa.\\nThe limitation of this approach is the requirement for one input anchor set per target meaning.\\nPreparing such input for all possible meanings in broad domains would be a huge task.\\nAs will be explained below, our method avoids this limitation by finding all anchor sets automatically in an unsupervised manner.\\nFinally, (Lin and Pantel, 2001) present a notably different approach that relies on matching separately single anchors.\\nThey limit the allowed structure of templates only to paths in dependency parses connecting two anchors.\\nThe algorithm constructs for each possible template two feature vectors, representing its co-occurrence statistics with the two anchors.\\nTwo templates with similar vectors are suggested as paraphrases (termed inference rule).\\nMatching of single anchors relies on the general distributional similarity principle and unlike the other methods does not require redundancy of sets of multiple anchors.\\nConsequently, a much larger number of paraphrases can be found in a regular corpus.\\nLin and Pantel report experiments for 9 templates, in which their system extracted 10 correct inference rules on average per input template, from 1GB of news data.\\nYet, this method also suffers from certain limitations: (a) it identifies only templates with pre-specified structures; (b) accuracy seems more limited, due to the weaker notion of similarity; and (c) coverage is limited to the scope of an available corpus.\\nTo conclude, several approaches exhaustively process different types of corpora, obtaining varying scales of output.\\nOn the other hand, the Web is a huge promising resource, but current Web-based methods suffer serious scalability constraints.\\nParaphrasing approaches learn different kinds of template structures.\\nInteresting algorithms are presented in (Pang et al., 2003; Barzilay and Lee, 2003).\\nThey learn linear patterns within similar contexts represented as finite state automata.\\nThree classes of syntactic template learning approaches are presented in the literature: learning ofpredicate argument templates (Yangarber et al., 2000), learning of syntactic chains (Lin and Pantel, 2001) and learning of sub-trees (Sudo et al., 2003).\\nThe last approach is the most general with respect to the template form.\\nHowever, its processing time increases exponentially with the size of the templates.\\nAs a conclusion, state of the art approaches still learn templates of limited form and size, thus restricting generality of the learning process.\\n\\nMotivated by prior experience, we identify two major goals for scaling Web-based acquisition of entailment relations: (a) Covering the broadest possible range of meanings, while requiring minimal input and (b) Keeping template structures as general as possible.\\nTo address the first goal we require as input only a phrasal lexicon of the relevant domain (including single words and multiword expressions).\\nBroad coverage lexicons are widely available or may be constructed using known term acquisition techniques, making it a feasible and scalable input requirement.\\nWe then aim to acquire entailment relations that include any of the lexicon\\u2019s entries.\\nThe second goal is addressed by a novel algorithm for extracting the most general templates being justified by the data.\\nFor each lexicon entry, denoted a pivot, our extraction method performs two phases: (a) extract promising anchor sets for that pivot (ASE, Section 3.1), and (b) from sentences containing the anchor sets, extract templates for which an entailment relation holds with the pivot (TE, Section 3.2).\\nExamples for verb pivots are: \\u2018acquire\\u2019, \\u2018fall to\\u2019, \\u2018prevent\\u2019.\\nWe will use the pivot \\u2018prevent\\u2019 for examples through this section.\\nBefore presenting the acquisition method we first define its output.\\nA template is a dependency parsetree fragment, with variable slots at some tree nodes (e.g.\\n\\u2018X s+_ prevent \\ufffd Y\\u2019).\\nAn entailment relation between two templates T1 and T2 holds if the meaning of T2 can be inferred from the meaning of T1 (or vice versa) in some contexts, but not necessarily all, under the same variable instantiation.\\nFor example, \\u2018X s+ prevent 0* Y\\u2019 entails \\u2018X s+_ reduce -* Y risk\\u2019 because the sentence \\u201caspirin reduces heart attack risk\\u201d can be inferred from \\u201caspirin prevents a first heart attack\\u201d.\\nOur output consists of pairs of templates for which an entailment relation holds.\\nThe goal of this phase is to find a substantial number of promising anchor sets for each pivot.\\nA good anchor-set should satisfy a proper balance between specificity and generality.\\nOn one hand, an anchor set should correspond to a sufficiently specific setting, so that entailment would hold between its different occurrences.\\nOn the other hand, it should be sufficiently frequent to appear with different entailing templates.\\nFinding good anchor sets based on just the input pivot is a hard task.\\nMost methods identify good repeated anchors \\u201cin retrospect\\u201d, that is after processing a full corpus, while previous Web-based methods require at least one good anchor set as input.\\nGiven our minimal input, we needed refined criteria that identify a priori the relatively few promising anchor sets within a sample of pivot occurrences.\\nThe ASE algorithm (presented in Figure 1) performs 4 main steps.\\nSTEP (1) creates a complete template, called the pivot template and denoted Tp, for the input pivot, denoted P. Variable slots are added for the major types of syntactic relations that interact with P, based on its syntactic type.\\nThese slots enable us to later match Tp with other templates.\\nFor verbs, we add slots for a subject and for an object or a modifier (e.g.\\n\\u2018X s+_ prevent \\ufffd Y\\u2019).\\nSTEP (2) constructs asample corpus, denoted S, for the pivot template.\\nSTEP (2.A) utilizes a Web search engine to initialize S by retrieving sentences containing P. The sentences are parsed by the MINIPAR dependency parser (Lin, 1998), keeping only sentences that contain the complete syntactic template Tp (with all the variables instantiated).\\nSTEP (2.B) identifies phrases that are statistically associated with Tp in S. We test all noun-phrases in S , discarding phrases that are too common on the Web (absolute frequency higher than a threshold MAXPHRASEF), such as \\u201cdesire\\u201d.\\nThen we select the N phrases with highest tf \\u00b7idf score1.\\nThese phrases have a strong collocation relationship with the pivot P and are likely to indicate topical (rather than anecdotal) occurrences of P. For example, the phrases \\u201cpatient\\u201d and \\u201cAmerican Dental Association\\u201d, which indicate contexts of preventing health problems, were selected for the pivot \\u2018prevent\\u2019.\\nFi1Here, tf \\u00b7idf = freqS(X) \\u00b7 log (freqN (X) ) where freqS(X) is the number of occurrences in S containing X, N is the total number of Web documents, and freqW (X) is the number of Web documents containing X. nally, STEP (2.C) expands S by querying the Web with the both P and each of the associated phrases, adding the retrieved sentences to S as in step (2.a).\\nSTEP (3) extracts candidate anchor sets for Tp.\\nFrom each sentence in S we try to generate one candidate set, containing noun phrases whose Web frequency is lower than MAXPHRASEF.\\nSTEP (3.A) extracts slot anchors \\u2013 phrases that instantiate the slot variables of Tp.\\nEach anchor is marked with the corresponding slot.\\nFor example, the anchors {antibioticssubj\\u2190 , miscarriage obj\\u2190} were extracted from the sentence \\u201cantibiotics in pregnancy prevent miscarriage\\u201d.\\nSTEP (3.B) tries to extend each candidate set with one additional context anchor, in order to improve its specificity.\\nThis anchor is chosen as the highest tf \\u00b7idf scoring phrase in the sentence, if it exists.\\nIn the previous example, \\u2018pregnancy\\u2019 is selected.\\nSTEP (4) filters out bad candidate anchor sets by two different criteria.\\nSTEP (4.A) maintains only candidates with absolute Web frequency within a threshold range [MINSETF, MAXSETF], to guarantee an appropriate specificity-generality level.\\nSTEP (4.B) guarantees sufficient (directional) association between the candidate anchor set c and Tp, by estimating where freqW is Web frequency and P is the pivot.\\nWe maintain only candidates for which this probability falls within a threshold range [SETMINP, SETMAXP].\\nHigher probability often corresponds to a strong linguistic collocation between the candidate and Tp, without any semantic entailment.\\nLower probability indicates coincidental cooccurrence, without a consistent semantic relation.\\nThe remaining candidates in S become the input anchor-sets for the template extraction phase, for example, {Aspirinsubj\\u2190 , heart attackobj\\u2190} for \\u2018prevent\\u2019.\\nThe Template Extraction algorithm accepts as its input a list of anchor sets extracted from ASE for each pivot template.\\nThen, TE generates a set of syntactic templates which are supposed to maintain an entailment relationship with the initial pivot template.\\nTE performs three main steps, described in the following subsections: For each input anchor set, TE acquires from the Web a sample corpus of sentences containing it.\\nFor example, a sentence from the sample corpus for {aspirin, heart attack} is: \\u201cAspirin stops heart attack?\\u201d.\\nAll of the sample sentences are then parsed with MINIPAR (Lin, 1998), which generates from each sentence a syntactic directed acyclic graph (DAG) representing the dependency structure of the sentence.\\nEach vertex in this graph is labeled with a word and some morphological information; each graph edge is labeled with the syntactic relation between the words it connects.\\nTE then substitutes each slot anchor (see section 3.1) in the parse graphs with its corresponding slot variable.\\nTherefore, \\u201cAspirin stops heart attack?\\u201d will be transformed into \\u2018X stop Y\\u2019.\\nThis way all the anchors for a certain slot are unified under the same variable name in all sentences.\\nThe parsed sentences related to all of the anchor sets are subsequently merged into a single set of parse graphs S = {P1, P2, ... , Pn} (see P1 and P2 in Figure 2).\\nThe core of TE is a General Structure Learning algorithm (GSL) that is applied to the set of parse graphs S resulting from the previous step.\\nGSL extracts single-rooted syntactic DAGs, which are named spanning templates since they must span at least over Na slot variables, and should also appear in at least Nr sentences from S (In our experiments we set Na=2 and Nr=2).\\nGSL learns maximal most general templates: they are spanning templates which, at the same time, (a) cannot be generalized by further reduction and (b) cannot be further extended keeping the same generality level.\\nIn order to properly define the notion of maximal most general templates, we introduce some formal definitions and notations.\\nDEFINITION: For a spanning template t we define a sentence set, denoted with \\u03c3(t), as the set of all parsed sentences in S containing t. For each pair of templates t1 and t2, we use the notation t1 :\\ufffd t2 to denote that t1 is included as a subgraph or is equal to t2.\\nWe use the notation t1 \\u227a t2 when such inclusion holds strictly.\\nWe define T(S) as the set of all spanning templates in the sample S. DEFINITION: A spanning template t E T (S) is maximal most general if and only if both of the following conditions hold: Condition A ensures that the extracted templates do not contain spanning sub-structures that are more \\u201dgeneral\\u201d (i.e. having a larger sentence set); condition B ensures that the template cannot be further enlarged without reducing its sentence set.\\nGSL performs template extraction in two main steps: (1) build a compact graph representation of all the parse graphs from S; (2) extract templates from the compact representation.\\nA compact graph representation is an aggregate graph which joins all the sentence graphs from S ensuring that all identical spanning sub-structures from different sentences are merged into a single one.\\nTherefore, each vertex v (respectively, edge e) in the aggregate graph is either a copy of a corresponding vertex (edge) from a sentence graph Pi or it represents the merging of several identically labeled vertices (edges) from different sentences in S. The set of such sentences is defined as the sentence set of v (e), and is represented through the set of index numbers of related sentences (e.g.\\n\\u201c(1,2)\\u201d in the third tree of Figure 2).\\nWe will denote with Gi the compact graph representation of the first i sentences in S. The parse trees P1 and P2 of two sentences and their related compact representation G2 are shown in Figure 2.\\nBuilding the compact graph representation The compact graph representation is built incrementally.\\nThe algorithm starts with an empty aggregate graph G0 and then merges the sentence graphs from S one at a time into the aggregate structure.\\nLet\\u2019s denote the current aggregate graph with Gi_1(Vg, Eg) and let Pi(Vp, Ep) be the parse graph which will be merged next.\\nNote that the sentence set of Pi is a single element set W. During each iteration a new graph is created as the union of both input graphs: Gi = Gi_1 U Pi.\\nThen, the following merging procedure is performed on the elements of Gi ated and added to Gi.\\nThe new vertex takes the same label and holds a sentence set which is formed from the sentence set of vg by adding i to it.\\nStill with reference to Figure 2, the generalized vertices in G2 are \\u2018X\\u2019, \\u2018Y\\u2019 and \\u2018stop\\u2019.\\nThe algorithm connects the generalized vertex vnew g with all the vertices which are connected with vg and vp.\\nAs an optimization step, we merge only vertices and edges that are included in equal spanning templates.\\nExtracting the templates GSL extracts all maximal most general templates from the final compact representation Gn using the following sub-algorithm: In Figure 2 the maximal most general template in obj As a last step, names and numbers are filtered out from the templates.\\nMoreover, TE removes those templates which are very long or which appear with just one anchor set and in less than four sentences.\\nFinally, the templates are sorted first by the number of anchor sets with which each template appeared, and then by the number of sentences in which they appeared.\\n\\nWe evaluated the results of the TE/ASE algorithm on a random lexicon of verbal forms and then assessed its performance on the extracted data through human-based judgments.\\nThe test set for human evaluation was generated by picking out 53 random verbs from the 1000 most frequent ones found in a subset of the Reuters corpus2.\\nFor each verb entry in the lexicon, we provided the judges with the corresponding pivot template and the list of related candidate entailment templates found by the system.\\nThe judges were asked to evaluate entailment for a total of 752 templates, extracted for 53 pivot lexicon entries; Table 1 shows a sample of the evaluated templates; all of them are clearly good and were judged as correct ones. included in the evaluation test set.\\nConcerning the ASE algorithm, threshold parameters3 were set as PHRASEMAXF=107, SETMINF=102, SETMAXF=105, SETMINP=0.066, and SETMAXP=0.666.\\nAn upper limit of 30 was imposed on the number of possible anchor sets used for each pivot.\\nSince this last value turned out to be very conservative with respect to system coverage, we subsequently attempted to relax it to 50 (see Discussion in Section 4.3).\\nFurther post-processing was necessary over extracted data in order to remove syntactic variations referring to the same candidate template (typically passive/active variations).\\nThree possible judgment categories have been considered: Correct if an entailment relationship in at least one direction holds between the judged template and the pivot template in some non-bizarre context; Incorrect if there is no reasonable context and variable instantiation in which entailment holds; No Evaluation if the judge cannot come to a definite conclusion.\\nEach of the three assessors (referred to as J#1, J#2, and J#3) issued judgments for the 752 different templates.\\nCorrect templates resulted to be 283, 313, and 295 with respect to the three judges.\\nNo evaluation\\u2019s were 2, 0, and 16, while the remaining templates were judged Incorrect.\\nFor each verb, we calculate Yield as the absolute number of Correct templates found and Precision as the percentage of good templates out of all extracted templates.\\nObtained Precision is 44.15%, averaged over the 53 verbs and the 3 judges.\\nConsidering Low Majority on judges, the precision value is 42.39%.\\nAverage Yield was 5.5 templates per verb.\\nThese figures may be compared (informally, as data is incomparable) with average yield of 10.1 and average precision of 50.3% for the 9 \\u201cpivot\\u201d templates of (Lin and Pantel, 2001).\\nThe comparison suggests that it is possible to obtain from the (very noisy) web a similar range of precision as was obtained from a clean news corpus.\\nIt also indicates that there is potential for acquiring additional templates per pivot, which would require further research on broadening efficiently the search for additional web data per pivot.\\nAgreement among judges is measured by the Kappa value, which is 0.55 between J#1 and J#2, 0.57 between J#2 and J#3, and 0.63 between J#1 and J#3.\\nSuch Kappa values correspond to moderate agreement for the first two pairs and substantial agreement for the third one.\\nIn general, unanimous agreement among all of the three judges has been reported on 519 out of 752 templates, which corresponds to 69%.\\nOur algorithm obtained encouraging results, extracting a considerable amount of interesting templates and showing inherent capability of discovering complex semantic relations.\\nConcerning overall coverage, we managed to find correct templates for 86% of the verbs (46 out of 53).\\nNonetheless, presented results show a substantial margin of possible improvement.\\nIn fact yield values (5.5 Low Majority, up to 24 in best cases), which are our first concern, are inherently dependent on the breadth of Web search performed by the ASE algorithm.\\nDue to computational time, the maximal number of anchor sets processed for each verb was held back to 30, significantly reducing the amount of retrieved data.\\nIn order to further investigate ASE potential, we subsequently performed some extended experiment trials raising the number of anchor sets per pivot to 50.\\nThis time we randomly chose a subset of 10 verbs out of the less frequent ones in the original main experiment.\\nResults for these verbs in the main experiment were an average Yield of 3 and an average Precision of 45.19%.\\nIn contrast, the extended experiments on these verbs achieved a 6.5 Yield and 59.95% Precision (average values).\\nThese results are indeed promising, and the substantial growth in Yield clearly indicates that the TE/ASE algorithms can be further improved.\\nWe thus suggest that the feasibility of our approach displays the inherent scalability of the TE/ASE process, and its potential to acquire a large entailment relation KB using a full scale lexicon.\\nA further improvement direction relates to template ranking and filtering.\\nWhile in this paper we considered anchor sets to have equal weights, we are also carrying out experiments with weights based on cross-correlation between anchor sets.\\n\\nWe have described a scalable Web-based approach for entailment relation acquisition which requires only a standard phrasal lexicon as input.\\nThis minimal level of input is much simpler than required by earlier web-based approaches, while succeeding to maintain good performance.\\nThis result shows that it is possible to identify useful anchor sets in a fully unsupervised manner.\\nThe acquired templates demonstrate a broad range of semantic relations varying from synonymy to more complicated entailment.\\nThese templates go beyond trivial paraphrases, demonstrating the generality and viability of the presented approach.\\nFrom our current experiments we can expect to learn about 5 relations per lexicon entry, at least for the more frequent entries.\\nMoreover, looking at the extended test, we can extrapolate a notably larger yield by broadening the search space.\\nTogether with the fact that we expect to find entailment relations for about 85% of a lexicon, it is a significant step towards scalability, indicating that we will be able to extract a large scale KB for a large scale lexicon.\\nIn future work we aim to improve the yield by increasing the size of the sample-corpus in a qualitative way, as well as precision, using statistical methods such as supervised learning for better anchor set identification and cross-correlation between different pivots.\\nWe also plan to support noun phrases as input, in addition to verb phrases.\\nFinally, we would like to extend the learning task to discover the correct entailment direction between acquired templates, completing the knowledge required by practical applications.\\nLike (Lin and Pantel, 2001), learning the context for which entailment relations are valid is beyond the scope of this paper.\\nAs stated, we learn entailment relations holding for some, but not necessarily all, contexts.\\nIn future work we also plan to find the valid contexts for entailment relations.\\n\\nThe authors would like to thank Oren Glickman (Bar Ilan University) for helpful discussions and assistance in the evaluation, Bernardo Magnini for his scientific supervision at ITC-irst, Alessandro Vallin and Danilo Giampiccolo (ITC-irst) for their help in developing the human based evaluation, and Prof. Yossi Matias (Tel-Aviv University) for supervising the first author.\\nThis work was partially supported by the MOREWEB project, financed by Provincia Autonoma di Trento.\\nIt was also partly carried out within the framework of the ITC-IRST (TRENTO, ITALY) \\u2013 UNIVERSITY OF HAIFA (ISRAEL) collaboration project.\\nFor data visualization and analysis the authors intensively used the CLARK system (www.bultreebank.org) developed at the Bulgarian Academy of Sciences.\\n\",\n          \"\\nExploiting Diverse Knowledge Sources Via Maximum Entropy In Named Entity Recognition\\n\\nThis paper describes a novel statistical namedentity (i.e.\\n&quot;proper name&quot;) recognition system built around a maximum entity framework.\\nBy working within the framework of maximum entropy theory and utilizing a flexible object-based architecture, the system is able to make use of an extraordinarily diverse range of knowledge sources in making its tagging decisions.\\nThese knowledge sources include capitalization features, lexical features, features indicating the current section of text (i.e. headline or main body), and dictionaries of single or multi-word terms.\\nThe purely statistical system contains no hand-generated patterns and achieves a result comparable with the best statistical systems.\\nHowever, when combined with other handcoded systems, the system achieves scores that exceed the highest comparable scores thus-far published.\\n\\nNamed entity recognition is one of the simplest of the common message understanding tasks.\\nThe objective is to identify and categorize all members of certain categories of &quot;proper names&quot; from a given corpus.\\nThe specific test bed which will be the subject of this paper is that of the Seventh Message Understanding Conference (MUC-7), in which the task was to identify &quot;names&quot; falling into one of seven categories: person, organization, location, date, time, percentage, and monetary amount.\\nThis paper describes a new system called &quot;Maximum Entropy Named Entity&quot; or &quot;MENE&quot; (pronounced &quot;meanie&quot;).\\nBy working within the framework of maximum entropy theory and utilizing a flexible object-based architecture, the system is able to make use of an extraordinarily diverse range of knowledge sources in making its tagging decision.\\nThese knowledge sources include capitalization features, lexical features, and features indicating the current section of text.\\nIt makes use of a broad array of dictionaries of useful single or multi-word terms such as first names, company names, and corporate suffixes, and automatically handles cases where words are in more than one dictionary.\\nOur dictionaries required no manual editing and were either downloaded from the web or were simply &quot;obvious&quot; lists entered by hand.\\nThis system, built from off-the-shelf knowledge sources, contained no hand-generated patterns and achieved a result which is comparable with that of the best statistical systems.\\nFurther experiments showed that when combined with handcoded systems from NYU, the University of Manitoba, and IsoQuest, Inc., MENE was able to generate scores which exceeded the highest scores thus-far reported by any system on a MUC evaluation.\\nGiven appropriate training data, we believe that this system is highly portable to other domains and languages and have already achieved good results on upper-case English.\\nWe also feel that there are plenty of avenues to explore in enhancing the system's performance on English-language newspaper text.\\n\\nGiven a tokenization of a test corpus and a set of n (for MUC-7, n = 7) tags which define the name categories of the task at hand, the problem of named entity recognition can be reduced to the problem of assigning one of 4n + 1 tags to each token.\\nFor any particular tag x from the set of n tags, we could be in one of 4 states: x_start, x_continue, x_end, and x_unique.\\nIn addition, a token could be tagged as &quot;other&quot; to indicate that it is not part of a named entity.\\nFor instance, we would tag the phrase [Jerry Lee Lewis flew to Paris] as [person_start, person_continue, person_end, other, other, location_unique].\\nThis approach is essentially the same as (Sekine et al., 1998).\\nThe 29 tags of MUC-7 form the space of &quot;futures&quot; for a maximum entropy formulation of our N.E. problem.\\nA maximum entropy solution to this, or any other similar problem allows the computation of p(flh) for any f from the space of possible futures, F, for every h from the space of possible histories, H. A &quot;history&quot; in maximum entropy is all of the conditioning data which enables you to make a decision among the space of futures.\\nIn the named entity problem, we could reformulate this in terms of finding the probability of f associated with the token at index tin the test corpus as: P(f Iht) = P f Itest corpus relative to token t ) ( Information derivable from the ) The computation of p(f1h) in M.E. is dependent on a set of &quot;features&quot; which, hopefully, are helpful in making a prediction about the future.\\nLike most current M.E. modeling efforts in computational linguistics we restrict ourselves to features which are binary functions of the history and future.\\nFor instance, one of our features is Here &quot;current-token-capitalized(h)&quot; is a binary function which returns true if the &quot;current token&quot; of the history h (the token whose tag we are trying to determine) has an initial capitalized letter.\\nGiven a set of features and some training data, the maximum entropy estimation process produces a model in which every feature gi has associated with it a parameter ai.\\nThis allows us to compute the conditional probability as follows (Berger et al., 1996): The maximum entropy estimation technique guarantees that for every feature gi, the expected value of gi according to the M.E. model will equal the empirical expectation of gi in the training corpus.\\nIn other words: Here P is an empirical probability and PmE is the probability assigned by the M.E. model.\\nMore complete discussions of M.E. as applied to computational linguistics, including a description of the M.E. estimation procedure can be found in (Berger et al., 1996) and (Della Pietra et al., 1995).\\nThe following are some additional references which are useful as introductions and examples of applications: (Ratnaparkhi, 1997b) (Ristad, 1.998) (Jaynes, 1996).\\nAs many authors have remarked, though, the most useful thing about maximum entropy modeling is that it allows the modeler to concentrate on finding the features that characterize the problem while letting the M.E. estimation routine worry about assigning the relative weights to the features.\\n\\nMENE consists of a set of C++ and Pen l modules which forms a wrapper around a publicly available M.E. toolkit (Ristad, 1998) which computes the values of the a parameters of equation 2 from a pair of training files created by MENE.\\nMENE's flexibility is due to its object-based treatment of the three essential components of a maximum entropy system: histories, futures, and features (Borthwick et al., 1997).\\nHistory objects in MENE act as containers for a list of &quot;history views&quot;.\\nThe history view classes each represent a different type of information about the history object.\\nWhen the features attempt to determine whether or not they fire on a given history, they request an appropriate history view object from the history object and then query the history view object to determine whether their firing conditions are satisfied.\\nNote that these history views generally hold information about a limited window around the current token.\\nIf the current token is denoted as wo, then our model only holds information about tokens w1 for all history views except the lexical ones.\\nFor these views, the window is w-2 \\u2022 \\u2022 \\u2022 W. Future objects, on the other hand, are trivial in that their only piece of data is an integer indicating which of the 29 members of the future space they represent.\\n\\nFeatures are implemented as binary valued functions which query the history and future objects to determine whether or not they &quot;fire&quot;.\\nIn the following sections, we will look at each of MENE's feature classes in turn.\\nWhile all of MENE's features have binary-valued output, the &quot;binary&quot; features are features whose associated history-view can be considered to be either on or off for a given token.\\nExamples are &quot;the token begins with a capitalized letter&quot; or &quot;the token is a four-digit number&quot;.\\nEquation I gives an example of a binary feature.\\nThe 11 binary history-views used by MENE's binary features are very similar to those used in BBN's Nymble/Identifinder system (Bikel et al., 1997) with two exceptions: b, when the (history,future) space on which feature b activates must be a subset of the space for feature a, it can be shown that the M.E. model will yield the same results whether a and b are included as features or if (a \\u2014 b) and b are features.\\nConsequently, MENE allows all features to fire in overlapping cases.\\nFor instance, in MENE the initial cap features activate on the histories &quot;Clinton&quot;, &quot;IBM&quot;, and &quot;ValuJet&quot; while in Nymble the feature would only be active on &quot;Clinton&quot; because the &quot;AllCap&quot; feature would take precedence on &quot;IBM&quot; and an &quot;Initial-and-internal-cap&quot; feature would take precedence on &quot;ValuJet&quot;.\\nTo create a lexical history view, the tokens at w-2 \\u2022 \\u2022 \\u2022 w2 are compared with a vocabulary and their vocabulary indices are recorded.\\nFor a given training corpus. we define the vocabulary to be all tokens with a count of three or more.\\nWords not found in the vocabulary are assigned a distinguished &quot;Unknown&quot; index.\\nLexical feature example: A more subtle feature picked up by MENE: preceding word is &quot;to&quot; and future is &quot;location_unique&quot;.\\nGiven the domain of the MUC-7 training data (aviation disasters), &quot;to&quot; is a weak indicator, but a real one.\\nThis is an example of a feature which MENE can make use of but which the constructor of a handcoded system would probably regard as too risky to incorporate.\\nThis feature, in conjunction with other weak features, can allow MENE to pick up names that other systems might miss.\\nAs discussed later, these features are automatically acquired and the system can attain a very high level of performance using these features alone.\\nThis is encouraging since these lexical features are not dependent on any external knowledge source or linguistic intuition and thus are completely portable to new domains.\\nThe New York Times articles which constituted the MUC-7 test and training corpora were composed of six distinct sections including &quot;Date&quot;, &quot;Preamble&quot;, and &quot;Text&quot;.\\nSection features activate according to which of these sections the current token is in.\\nExample feature: if Section-View(tokeno(h)) .9(h, n = 1 : = &quot;Preamble&quot; and f = person_unique 0 : else Activation example: CLINTON WARNS HUSSEIN ABOUT IRAQI DEFIANCE.\\nNote that, assuming that this headline is in the preamble, the above feature will fire on all of these words.\\nOf course, this feature's prediction will only be correct on &quot;CLINTON&quot; and &quot;HUSSEIN&quot;.\\nSection features establish the background probability of the occurrence of the different futures.\\nFor instance, in NYU's evaluation system, the a value assigned to the feature which predicts &quot;other&quot; given a current section of &quot;main body of text&quot; is 7.9 times stronger than the feature which predicts &quot;person_unique&quot; in the same section.\\nThus the system predicts &quot;other&quot; by default.\\nOn the other hand, in the preamble (which contains headline, author, etc. information), the feature predicting &quot;other&quot; is much weaker in most cases.\\nIt is only about 2.6 times as strong as &quot;organization_start&quot; and &quot;organization_end&quot;, for instance.\\nMulti-word dictionaries are a key element of MENE.\\nEach entry in a MENE dictionary consists of a term which is one or more tokens long.\\nDictionaries can be case-sensitive or not on a dictionary-bydictionary basis.\\nA pre-processing step summarizes the information in the dictionary on a token-bytoken basis by assigning to every token one of the following five tags for each dictionary: start, continue, end, unique, other.\\nI.e. if &quot;British Airways&quot; was in our dictionary, a dictionary feature would see the phrase &quot;on British Airways Flight 962&quot; as &quot;other, start, end, other, other&quot;.\\nTable 1 lists the dictionaries. used by MENE in the MUC-7 evaluation.\\nBelow is an example of a dictionary feature: if First-Name-DictionaryNote that, similar to the case of overlapping binary features, we don't have to worry about words appearing in the dictionary which are commonly used in another sense.\\nI.e. we can leave dangerouslooking names like &quot;April&quot; in the first-name dictionary because whenever the first-name feature fires on &quot;April&quot;, the lexical and date-dictionary features for &quot;April&quot; will also fire and, assuming that the use of April as &quot;date&quot; exceeded the use of April as person_start or person_unique, we can expect that the lexical feature will have a high enough a value to outweigh the first-name-dictionary feature.\\nThis was confirmed in our test runs: no instance of &quot;April&quot; was tagged as a name, including one case, &quot;The death of Ron Brown in April in a similar plane crash ...&quot; which could be thought of as somewhat tricky because the month was not followed by a specific date.\\nNote that the system isn't foolproof: if a &quot;dangerous&quot; dictionary word appeared in only one dictionary and did not appear often enough in the training corpus to be included in the vocabulary, but did appear in the test corpus, we would probably mistag it.\\nFor NYU's official entry in the MUC-7 evaluation, MENE took in the output of an enhanced version of the more traditional, hand-coded &quot;Proteus&quot; namedentity tagger which we entered in MUC-6(Grishman, 1995).\\nIn addition, subsequent to the evaluation, the University of Manitoba (Lin, 1998) and IsoQuest, Inc. (Krupka and Hausman, 1998) shared with us the outputs of their systems on our training corpora as well as on various test corpora.\\nThe output sent to us was the standard MUC-7 output, so our collaborators didn't have to do any special processing for us.\\nThese systems were incorporated into MENE as simply three more history views by the following 2 step process: The result of all this is that the &quot;futures&quot; produced by the three external systems become three &quot;external system histories&quot; for MENE.\\nHere is an It is important to note that MENE has features which predict a different future than the future predicted by the external system.\\nThis can be seen as the process by which MENE learns the errors which the external system is likely to make.\\nAn example of this is that on the evaluation system the feature which predicted person_unique given a tag of person_unique by Proteus had only a 76% higher weight than the feature which predicted person_start given person_unique.\\nIn other words, Proteus had a tendency to chop off multi-word names at the first word.\\nMENE learned this and made it easy to override Proteus in this way.\\nIn fact, an analysis of the differences between the Proteus output and the MENE + Proteus output turned up a significant number of instances in which MENE extended or contracted name boundaries in this way.\\nGiven proper training data, MENE can pinpoint and selectively correct the weaknesses of a handcoded system.\\n\\nMENE currently has no direct ability to learn compound features or &quot;patterns&quot;\\u2014the &quot;history&quot; side of a lexical feature activates based on only a single word, for instance.\\nA sort of pattern-like ability comes into the system from multiple features firing at once.\\nI.e. to predict that &quot;York&quot; in the name &quot;New York&quot; is the end of a location, we will have two features firing: one predicts location_end when token_i is &quot;new&quot;.\\nThe other predicts location_end when tokeno is &quot;york&quot;.\\nNevertheless, it is possible that compound features would behave differently from two simultaneously firing &quot;atomic&quot; features.\\nWe integrated this into the model in an ad hoc manner for the external system features, where we constructed features which essentially query the external system history and the section history simultaneously to determine whether they fire.\\nI.e. a particular feature might fire if Proteus predicts person_start, the current section is &quot;main body of text&quot;, and the future is &quot;person_start&quot;.\\nThis allows MENE to assign a lower a to a Proteus prediction in the preamble vs. a prediction in the main body of text.\\nProteus, like many hand-coded systems, is more accurate in the main body of the text than in headline-type material.\\nWe found that this compound feature gave the system slightly higher performance than we got when we just used section features and external system features separately.\\nIt seems reasonable that adding an ability to handle fully general compound features (i.e. feature A fires if features B and C both fire) would improve system performance based on this limited experiment.\\nIn addition to allowing us to predict futures based on multi-word patterns, it would also let us use other promising combinations of features such as distinguishing between capitalization in a headline vs. in the main body of the text.\\nUnfortunately, this experiment will have to wait until we deploy a more sophisticated method of feature selection, as discussed in the next section.\\n\\nFeatures are chosen by a very simple method.\\nAll possible features from the classes we want included in our model are put into a &quot;feature pool&quot;.\\nFor instance, if we want lexical features in our model which activate on a range of token_2 . token2, our vocabulary has a size of V, and we have 29 futures, we will add (5.\\n(V +1) \\u2022 29) lexical features to the pool.\\nThe V + 1 term comes from the fact that we include all words in the vocabulary plus the unknown word.\\nFrom this pool, we then select all features which fire at least three times on the training corpus.\\nNote that this algorithm is entirely free of human intervention.\\nOnce the modeler has selected the classes of features, MENE will both select all the relevant features and train the features to have the proper weightings.\\nWe deviate from this basic algorithm in three ways: 1.\\nWe exclude features which activate on some sort of &quot;default&quot; value of a history view.\\nMany history views have some sort of default value which they display for the vast majority of tokens.\\nFor instance, a first-name-dictionary history view would say that the current token is not a name in over 99% of the cases.\\nRather than adding features which activate both when the token in question is and when it is not a first name, we only include features which activate when the token is a first name.\\nA feature which activated when a token was not a first name, while theoretically not harmful, would have practical disadvantages.\\nFirst of all, the feature would probably be redundant, because if the frequency of a future given a first-namedictionary hit is constrained (by equation 4), then the future frequency given a non-hit is also implicitly constrained.\\nSecondly, since this feature would fire on nearly every token, it would slow down run-time performance.\\nFinally, while maximum entropy models are designed to handle feature overlap, a very high degree of overlap requires more iterations of the maximum entropy estimation routine and can lead to numerical difficulties (Ristad, 1998).\\nLike the previous heuristic, this is based on the idea that features predicting named entities are more useful than features predicting the default.\\nNote that this method of feature selection would probably break down if we tried to incorporate general compound features into our model as described in the previous section.\\nThe model currently has about 24,000 features when trained on 350 articles of text.\\nIf we even considered all pairs of features as potential compound features, the 0(n2) compound features which we could build from our atomic features would undoubtedly yield an unacceptable slowdown in the model's performance.\\nClearly a more sophisticated feature selection routine such as the ones in (Berger et al., 1996), or (Berger and Printz, 1998) would be required in this case.\\n\\nAfter having trained the features of an M.E. model and assigned the proper weight (a values) to each of the features, decoding (i.e.\\n&quot;marking up&quot;) a new piece of text is a fairly simple process: The Viterbi search is necessary because simply taking the highest-probability future assigned to each token would result in incompatible assignments.\\nFor instance, an assignment of [person-start, location_end] to two consecutive tokens would be invalid.\\nThe Viterbi search finds the highest probability path in which there are no two tokens in which the second one cannot follow the first, as defined by a table of all such invalid transitions (a similar approach to (Sekine et al.. 1998)).\\n\\nMENE's maximum entropy training algorithm gives it reasonable performance with moderate-sized training corpora or few information sources, while allowing it to really shine when more training data and information sources are added.\\nTable 2 shows MENE's performance on the MUC-7 &quot;dry run&quot; corpus, which consisted of 25 articles mostly on the topic of aviation disasters.\\nAll systems shown were trained on 350 articles on the same domain (this training corpus consisted of about 270,000 words, which our system turned into 321,000 tokens).\\nNote the smooth progression of the scores as more data is added to the system.\\nAlso note that, when combined under MENE, the three weakest systems, MENE, Proteus, and Manitoba outperform the strongest single system, IsoQuest's.\\nFinally, the top score of 97.12 from combining all three systems is a very strong result.\\nOn a different set of data, the MUC-7 formal run data, the accuracy of the two human taggers who were preparing the answer key was tested and it was discovered that one of them had an F-Measure of 96.95 and the other of 97.60 (Marsh and Perzanowski, 1998).\\nAlthough we don't have human performance measures on the dry run test set, it seems that we have attained a result which is at least competitive with that of a human.\\nWe also did a series of runs to examine how the systems performed with different amounts of training data.\\nThese experiments are summarized in table 3.\\nNote the 97.38 all-systems result which we achieved by adding 75 articles from the formal-run test corpus to the basic 350-article training data.\\nIn addition to being an outstanding performance figure, this number shows MENE's responsiveness to good training material.\\nA few other conclusions can be drawn from this data.\\nFirst of all, MENE needs at least 20 articles of tagged training data to get acceptable performance on its own.\\nSecondly, there is a minimum amount of training data which is needed for MENE to improve an external system.\\nFor Proteus and the Manitoba system, this number seems to be about 80 articles, because they show a degradation of performance at 40.\\nSince the IsoQuest system was stronger to start with, MENE required 150 articles to show an improvement.\\nNote the anomaly in comparing the 250 and 350 article columns.\\nProteus shows only a very small gain and IsoQuest shows a deterioration.\\nThese last 100 articles added to the system were tagged by us at NYU, and we would humbly guess that we tagged them less carefully than the rest of the data which was tagged by BBN and Science Applications International Corporation (SAIC).\\nMENE has also been run against all-uppercase data.\\nOn this we achieved an F-measure of 88.19 for the MENE-only system and 91.38 for the MENE + Proteus system.\\nThe latter figure matches the best currently published result (Bikel et al., 1997) on within-domain all-caps data.\\nOn the other hand, we scored lower on all-caps than BBN's Identifinder in the MUC-7 formal evaluation for reasons which are probably similar to the ones discussed in section 9 in the comparison of our mixed case performances (Miller et al., 1998) (Borthwick et al., 1998).\\nWe have put very little effort into optimizing MENE on this type of corpus and believe that there is room for improvement here.\\nIn another experiment, we stripped out all features other than the lexical features and still achieved an F-measure of 88.13.\\nSince these features do not rely on any external knowledge sources and are automatically generated, this result is a strong indicator of MEN E's portability.\\nThe MUC-7 formal evaluation involved a shift in topic which was not communicated to the participants beforehand-the training data focused on airline disasters while the test data was on missile and rocket launches.\\nMENE fared much more poorly on this data than it did on the within-domain data quoted above, achieving an F-measure of only 88.80 for the MENE + Proteus system and 84.22 for the MENE-only system.\\nWhile 88.80 was still the fourth highest score out of the twelve participants in the evaluation, we feel that it is necessary to view this number as a cross-domain portability result rather than as an indicator of how the system can do on unseen data within its training domain.\\n\\\\Ve believe that if the system had been allowed to train on missile/rocket launch articles, its performance on these articles would have been much better.\\nMore MENE test results and discussion of the formal run can be found in (Borthwick et al., 1998).\\n\\nM.E. has been successfully applied to many other tasks in computational linguistics.\\nSome recent work for which there are solid comparable benchmarks is the work of Adwait Ratnaparkhi at the University of Pennsylvania.\\nHe has achieved state-of-the art results by applying M.E. to parsing (Ratnaparkhi, 1997a), part-of-speech tagging (Ratnaparkhi, 1996), and sentence-boundary detection (Reynar and Ratnaparkhi, 1997).\\nOther recent work has applied M.E. to language modeling (Rosenfeld, 1994), machine translation (Berger et al., 1996), and reference resolution (Kehler, 1997).\\nM.E. was first applied to named entity recognition at the MUC-7 conference by (Borthwick et al., 1998) and (Mikheev and Grover, 1998).\\nNote that part-of-speech tagging is, in many ways, a very similar task to that of named-entity recognition.\\nRatnaparkhi's tagger is similar to MENE, in that his features look at the surrounding two-word lexical context, but his system makes less use of dictionaries.\\nOn the other hand, his system looks at word suffixes and prefixes in the case of unknown words, which is something we haven't tried with MENE and looks at its own output by looking at its previous two tags when making its decision.\\nWe do this implicitly through our requirement that the futures we output be consistent, but we found that an attempt to do this more directly by building a consistency feature directly into the model had no effect on our results.\\nAt the MUC-7 conference, there were two other interesting systems using statistical techniques from the Language Technology Group/University of Edinborough (Mikheev and Grover, 1998) and BBN (Miller et al., 1998).\\nComparisons with the LTG system are difficult since it was a hybrid model in which the text was passed through a five-stage process, only three of which involved maximum entropy and over half of the system's recall came from the two non-statistical phases.\\nThe LTG system demonstrated superior performance on the formal run relative to the MENE-Proteus hybrid system (93.39 vs 88.80), but it isn't clear whether their advantage came from superior handcoded rules or superior statistical techniques, because their system is not as easily broken down into separate components as is MENE-Proteus.\\nIt is also possible that tighter system integration between the statistical and handcoded components was responsible for some of LTG's relative advantage, but note that MENE-Proteus appears to have an advantage over LTG in terms of portability.\\nWe are currently experimenting with porting MENE to Japanese, for instance, and expect that it could be combined with a pre-existing Japanese handcoded system, but it isn't clear that this could be done with the LTG system.\\nNevertheless, one of our avenues for future research is to look at tighter multi-system integration methods which won't compromise MEN E's essential portability.\\nTable 4 gives a comparison of BBN's HMMbased Identifinder (Miller et al., 1998) and NYU's MENE and MENE-Proteus systems on different training and test sets.\\nWe are not sure why MENEProteus was hurt more badly by the evaluationtime switch from aviation disaster articles to missue/rocket launch articles, but suspect that it may have been due to Identifinder's greater quantity and quality of training data.\\nBBN used 790,000 words of training data to our 321,000.\\nThe quality advantage may have come from selecting sentences from a larger corpus for their annotators to tag which were chosen so as to increase the variety of training data.\\nWhen MENE-only and Identifinder are compared training on the same number of articles and testing on within-domain data, Identifinder still has an edge.\\nWe speculate that this is due to the dynamic updating of Identifinder's vocabulary during decoding when person or organization names are recognized, which gives the system a sort of long-distance reference resolution which is lacking in MENE.\\nIn addition.\\nBBN's HMM-based system implictly predicts named entities based on consecutive pairs of words rather than based on single words, as is done in MENE, because each type of name has its own bigram language model.\\nIn the decoding process, the Viterbi algorithm chooses the sequence of names which yields the highest joint probability of names, words, and features associated with each word.\\nIn comparing the maximum entropy and HMMbased approaches to named entity recognition, we are hopeful that M.E. will turn out to be the better method in the end.\\nWe think it is possible that some of Identifinder's current advantage can be neutralized by simply adding the just-mentioned features to MENE.\\nOn the other hand, we have a harder time seeing how some of MENE's strengths can be integrated into an HMM-based system.\\nIt is not clear, for instance, how a wide variety of dictionaries could be added to Identifinder or whether the system could be combined with a handcoded system as was done with our system and the one from LTG.\\n\\nMENE is a very new, and, we feel, still immature system.\\nWork started in October, 1997, and the system described above was not in place until midFebruary.\\n1998.\\nWe believe that we can push the score of the MENE-only system higher by incorporating long-range reference-resolution on MENE's output.\\nWe are also missing a large number of acronyms which could be picked up by dynamically building them from entities which MENE had tagged elsewhere and then pulling that data in as a new class of feature.\\nThe other key element missing from the current system is a set of general compound features, which, as discussed above, would require the use of a more sophisticated feature selection algorithm.\\nAll three of these elements are present in systems such as Is\\u00b0Quest's (Kruplca and Hausman, 1998), and their absence from MENE probably explains much of the reason why the MENE-only system failed to perform at the state-of-the-art.\\nWe intend to add all of these elements to MENE in the near future to test this hypothesis.\\nNevertheless, we believe that we have already demonstrated some very useful results.\\nMENE is highly portable, as we have already demonstrated with our result on upper-case English text and even in its current state, its results are already comparable to that of the only other purely statistical English NE system which we are aware of (Miller et al., 1998).\\nAs shown with our result on running MENE with only the lexical features that it learns from the training corpus, porting MENE can be done with very little effort if appropriate training data is provided\\u2014it isn't even necessary to provide it with dictionaries to generate an acceptable result.\\nWe are working on a port to Japanese NE to further demonstrate MENE's flexibility.\\nHowever, we believe that the results on combining MENE with other systems are some of the most intriguing.\\nWe would hypothesize that, given sufficient training data, any handcoded system would benefit from having its output passed to MENE as a final step.\\nMENE also opens up new avenues for collaboration whereby different organizations could focus on different aspects of the problem of N.E. recognition with the maximum entropy system acting as an arbitrator.\\nMENE also offers the prospect of achieving very high performance with very little effort.\\nSince MENE starts out with a fairly high base score just on its own, we speculate that a MENE user could then construct a hand-coded system which only focused on MENE's weaknesses, while skipping the areas in which MENE is already strong.\\nFinally, one can imagine a user acquiring licenses to several different N.E. systems, generating some training data, and then combining it all under a MENE-like system.\\nWe have shown that this approach can yield performance which is competitive with that of a human tagger.\\n\\nWe would like to thank Troy Straszheim for writing the Viterbi search routine used in this work.\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 608,\n        \"samples\": [\n          \"Title: Coarse-To-Fine N-Best Parsing And MaxEnt Discriminative Reranking\\n\\nAbstract: Discriminative reranking is one method for constructing high-performance statistical parsers (Collins, 2000).A discriminative reranker requires a source of candidate parses for each sentence.This paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse-to-fine generative parser (Charniak, 2000).This method generates 50-best lists that are of substantially higher quality than previously obtainable.We used these parses as the input to a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002) that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91.0% on sentences of length 100 or less.\\n\\nConclusion: This paper has described a dynamic programming n-best parsing algorithm that utilizes a heuristic coarse-to-fine refinement of parses.Because the coarse-to-fine approach prunes the set of possible parse edges beforehand, a simple approach which enumerates the n-best analyses of each parse edge is not only practical but quite efficient.We use the 50-best parses produced by this algorithm as input to a MaxEnt discriminative reranker.The reranker selects the best parse from this set of parses using a wide variety of features.The system we described here has an f-score of 0.91 when trained and tested using the standard PARSEVAL framework.This result is only slightly higher than the highest reported result for this test-set, Bod\\u2019s (.907) (Bod, 2003).More to the point, however, is that the system we describe is reasonably efficient so it can be used for the kind of routine parsing currently being handled by the Charniak or Collins parsers.A 91.0 f-score represents a 13% reduction in fmeasure error over the best of these parsers.2 Both the 50-best parser, and the reranking parser can be found at ftp://ftp.cs.brown.edu/pub/nlparser/, named parser and reranker respectively.Acknowledgements We would like to thanks Michael Collins for the use of his data and many helpful comments, and Liang Huang for providing an early draft of his paper and very useful comments on our paper.Finally thanks to the National Science Foundation for its support (NSF IIS-0112432, NSF 9721276, and NSF DMS-0074276).\\n\",\n          \"Title: Scaling Web-Based Acquisition Of Entailment Relations\\n\\nAbstract: Paraphrase recognition is a critical step for natural language interpretation.Accordingly, many NLP applications would benefit from high coverage knowledge bases of paraphrases.However, the scalability of state-of-the-art paraphrase acquisition approaches is still limited.We present a fully unsupervised learning algorithm for Web-based extraction an extended model of paraphrases.We focus on increased scalability and generality with respect to prior work, eventually aiming at a full scale knowledge base.Our current implementation of the algorithm takes as its input a verb lexicon and for each verb searches the Web for related syntactic entailment templates.Experiments show promising results with respect to the ultimate goal, achieving much better scalability than prior Web-based methods.\\n\\nConclusion: We have described a scalable Web-based approach for entailment relation acquisition which requires only a standard phrasal lexicon as input.This minimal level of input is much simpler than required by earlier web-based approaches, while succeeding to maintain good performance.This result shows that it is possible to identify useful anchor sets in a fully unsupervised manner.The acquired templates demonstrate a broad range of semantic relations varying from synonymy to more complicated entailment.These templates go beyond trivial paraphrases, demonstrating the generality and viability of the presented approach.From our current experiments we can expect to learn about 5 relations per lexicon entry, at least for the more frequent entries.Moreover, looking at the extended test, we can extrapolate a notably larger yield by broadening the search space.Together with the fact that we expect to find entailment relations for about 85% of a lexicon, it is a significant step towards scalability, indicating that we will be able to extract a large scale KB for a large scale lexicon.In future work we aim to improve the yield by increasing the size of the sample-corpus in a qualitative way, as well as precision, using statistical methods such as supervised learning for better anchor set identification and cross-correlation between different pivots.We also plan to support noun phrases as input, in addition to verb phrases.Finally, we would like to extend the learning task to discover the correct entailment direction between acquired templates, completing the knowledge required by practical applications.Like (Lin and Pantel, 2001), learning the context for which entailment relations are valid is beyond the scope of this paper.As stated, we learn entailment relations holding for some, but not necessarily all, contexts.In future work we also plan to find the valid contexts for entailment relations.\\n\",\n          \"Title: Exploiting Diverse Knowledge Sources Via Maximum Entropy In Named Entity Recognition\\n\\nAbstract: This paper describes a novel statistical namedentity (i.e.&quot;proper name&quot;) recognition system built around a maximum entity framework.By working within the framework of maximum entropy theory and utilizing a flexible object-based architecture, the system is able to make use of an extraordinarily diverse range of knowledge sources in making its tagging decisions.These knowledge sources include capitalization features, lexical features, features indicating the current section of text (i.e. headline or main body), and dictionaries of single or multi-word terms.The purely statistical system contains no hand-generated patterns and achieves a result comparable with the best statistical systems.However, when combined with other handcoded systems, the system achieves scores that exceed the highest comparable scores thus-far published.\\n\\nConclusion: MENE's maximum entropy training algorithm gives it reasonable performance with moderate-sized training corpora or few information sources, while allowing it to really shine when more training data and information sources are added.Table 2 shows MENE's performance on the MUC-7 &quot;dry run&quot; corpus, which consisted of 25 articles mostly on the topic of aviation disasters.All systems shown were trained on 350 articles on the same domain (this training corpus consisted of about 270,000 words, which our system turned into 321,000 tokens).Note the smooth progression of the scores as more data is added to the system.Also note that, when combined under MENE, the three weakest systems, MENE, Proteus, and Manitoba outperform the strongest single system, IsoQuest's.Finally, the top score of 97.12 from combining all three systems is a very strong result.On a different set of data, the MUC-7 formal run data, the accuracy of the two human taggers who were preparing the answer key was tested and it was discovered that one of them had an F-Measure of 96.95 and the other of 97.60 (Marsh and Perzanowski, 1998).Although we don't have human performance measures on the dry run test set, it seems that we have attained a result which is at least competitive with that of a human.We also did a series of runs to examine how the systems performed with different amounts of training data.These experiments are summarized in table 3.Note the 97.38 all-systems result which we achieved by adding 75 articles from the formal-run test corpus to the basic 350-article training data.In addition to being an outstanding performance figure, this number shows MENE's responsiveness to good training material.A few other conclusions can be drawn from this data.First of all, MENE needs at least 20 articles of tagged training data to get acceptable performance on its own.Secondly, there is a minimum amount of training data which is needed for MENE to improve an external system.For Proteus and the Manitoba system, this number seems to be about 80 articles, because they show a degradation of performance at 40.Since the IsoQuest system was stronger to start with, MENE required 150 articles to show an improvement.Note the anomaly in comparing the 250 and 350 article columns.Proteus shows only a very small gain and IsoQuest shows a deterioration.These last 100 articles added to the system were tagged by us at NYU, and we would humbly guess that we tagged them less carefully than the rest of the data which was tagged by BBN and Science Applications International Corporation (SAIC).MENE has also been run against all-uppercase data.On this we achieved an F-measure of 88.19 for the MENE-only system and 91.38 for the MENE + Proteus system.The latter figure matches the best currently published result (Bikel et al., 1997) on within-domain all-caps data.On the other hand, we scored lower on all-caps than BBN's Identifinder in the MUC-7 formal evaluation for reasons which are probably similar to the ones discussed in section 9 in the comparison of our mixed case performances (Miller et al., 1998) (Borthwick et al., 1998).We have put very little effort into optimizing MENE on this type of corpus and believe that there is room for improvement here.In another experiment, we stripped out all features other than the lexical features and still achieved an F-measure of 88.13.Since these features do not rely on any external knowledge sources and are automatically generated, this result is a strong indicator of MEN E's portability.The MUC-7 formal evaluation involved a shift in topic which was not communicated to the participants beforehand-the training data focused on airline disasters while the test data was on missile and rocket launches.MENE fared much more poorly on this data than it did on the within-domain data quoted above, achieving an F-measure of only 88.80 for the MENE + Proteus system and 84.22 for the MENE-only system.While 88.80 was still the fourth highest score out of the twelve participants in the evaluation, we feel that it is necessary to view this number as a cross-domain portability result rather than as an indicator of how the system can do on unseen data within its training domain.\\\\Ve believe that if the system had been allowed to train on missile/rocket launch articles, its performance on these articles would have been much better.More MENE test results and discussion of the formal run can be found in (Borthwick et al., 1998).\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["df.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"DQHw_vfYuymq","executionInfo":{"status":"ok","timestamp":1731069256730,"user_tz":-330,"elapsed":426,"user":{"displayName":"21dcs022 BHARGAV","userId":"14054464433858683476"}},"outputId":"c5c89d67-edfb-4afc-b034-75902951f3e8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 608 entries, 0 to 607\n","Data columns (total 3 columns):\n"," #   Column   Non-Null Count  Dtype \n","---  ------   --------------  ----- \n"," 0   Index    608 non-null    int64 \n"," 1   Article  608 non-null    object\n"," 2   Summary  608 non-null    object\n","dtypes: int64(1), object(2)\n","memory usage: 14.4+ KB\n"]}]},{"cell_type":"code","source":["\n","wc= pd.DataFrame()\n","\n","wc['article'] = df['Article'].str.split().str.len()\n","wc['summary'] = df['Summary'].str.split().str.len()\n","\n","print(f\"Maximum word count:{ wc['article'].max()} average : {wc['article'].mean()} min : {wc['article'].min()}\")\n","\n","print(f\"Maximum word count:{ wc['summary'].max()} average : {wc['summary'].mean()} min : {wc['summary'].min()}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j1abXA2mvQbK","executionInfo":{"status":"ok","timestamp":1731069451533,"user_tz":-330,"elapsed":512,"user":{"displayName":"21dcs022 BHARGAV","userId":"14054464433858683476"}},"outputId":"21158de8-495a-4724-9e5f-1b3cac45d33f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Maximum word count:18867 average : 4003.159539473684 min : 748\n","Maximum word count:5591 average : 407.65625 min : 111\n"]}]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","**Preprocessing**\n","\n","---\n","\n"],"metadata":{"id":"tzpzJMBR1lYe"}},{"cell_type":"code","source":["import re\n","\n","def preprocess_text(text):\n","    text = text.replace('\\n', ' ').replace('\\r', ' ').strip()\n","    text = re.sub(r'[^\\w\\s.,!?;\\'\"-]', '', text)             #special character except punctuations\n","    text = re.sub(r'\\s+', ' ', text)\n","\n","    return text\n","\n","df['Article'] = df['Article'].apply(preprocess_text)\n","df['Summary'] = df['Summary'].apply(preprocess_text)\n","df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"collapsed":true,"id":"6MA3WwPW1uka","executionInfo":{"status":"ok","timestamp":1731093248848,"user_tz":-330,"elapsed":3184,"user":{"displayName":"21dcs022 BHARGAV","userId":"14054464433858683476"}},"outputId":"999ec97c-9a9c-40cf-d457-a220f7342c94"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   Index                                            Article  \\\n","0      1  D-Tree Grammars designed to share some of the ...   \n","1      2  Joint Learning Improves Semantic Role Labeling...   \n","2      3  Bilingually-Constrained Monolingual Shift-Redu...   \n","3      4  A Generative Constituent-Context Model For Imp...   \n","4      5  Word Association Norms Mutual Information And ...   \n","\n","                                             Summary  \n","0  Title D-Tree Grammars Abstract designed to sha...  \n","1  Title Joint Learning Improves Semantic Role La...  \n","2  Title Bilingually-Constrained Monolingual Shif...  \n","3  Title A Generative Constituent-Context Model F...  \n","4  Title Word Association Norms Mutual Informatio...  "],"text/html":["\n","  <div id=\"df-c01028c9-f4a9-4455-9764-83256be6e3b9\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Index</th>\n","      <th>Article</th>\n","      <th>Summary</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>D-Tree Grammars designed to share some of the ...</td>\n","      <td>Title D-Tree Grammars Abstract designed to sha...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>Joint Learning Improves Semantic Role Labeling...</td>\n","      <td>Title Joint Learning Improves Semantic Role La...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>Bilingually-Constrained Monolingual Shift-Redu...</td>\n","      <td>Title Bilingually-Constrained Monolingual Shif...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>A Generative Constituent-Context Model For Imp...</td>\n","      <td>Title A Generative Constituent-Context Model F...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>Word Association Norms Mutual Information And ...</td>\n","      <td>Title Word Association Norms Mutual Informatio...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c01028c9-f4a9-4455-9764-83256be6e3b9')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-c01028c9-f4a9-4455-9764-83256be6e3b9 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-c01028c9-f4a9-4455-9764-83256be6e3b9');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-323a5d10-ce51-40a4-826c-0fb66099f700\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-323a5d10-ce51-40a4-826c-0fb66099f700')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-323a5d10-ce51-40a4-826c-0fb66099f700 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 608,\n  \"fields\": [\n    {\n      \"column\": \"Index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 175,\n        \"min\": 1,\n        \"max\": 608,\n        \"num_unique_values\": 608,\n        \"samples\": [\n          110,\n          11,\n          185\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Article\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 608,\n        \"samples\": [\n          \"Coarse-To-Fine N-Best Parsing And MaxEnt Discriminative Reranking Discriminative reranking is one method for constructing high-performance statistical parsers Collins, 2000. A discriminative reranker requires a source of candidate parses for each sentence. This paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse-to-fine generative parser Charniak, 2000. This method generates 50-best lists that are of substantially higher quality than previously obtainable. We used these parses as the input to a MaxEnt reranker Johnson et al., 1999; Riezler et al., 2002 that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91.0 on sentences of length 100 or less. We describe a reranking parser which uses a regularized MaxEnt reranker to select the best parse from the 50-best parses returned by a generative parsing model. The 50-best parser is a probabilistic parser that on its own produces high quality parses; the maximum probability parse trees according to the parsers model have an f-score of 0.897 on section 23 of the Penn Treebank Charniak, 2000, which is still state-of-the-art. However, the 50 best i.e., the 50 highest probability parses of a sentence often contain considerably better parses in terms of f-score; this paper describes a 50-best parsing algorithm with an oracle f-score of 96.8 on the same data. The reranker attempts to select the best parse for a sentence from the 50-best list of possible parses for the sentence. Because the reranker only has to consider a relatively small number of parses per sentences, it is not necessary to use dynamic programming, which permits the features to be essentially arbitrary functions of the parse trees. While our reranker does not achieve anything like the oracle f-score, the parses it selects do have an f-score of 91.0, which is considerably better than the maximum probability parses of the n-best parser. In more detail, for each string s the n-best parsing algorithm described in section 2 returns the n highest probability parses Ys y1s, ... , yns together with the probability py of each parse y according to the parsers probability model. The number n of parses was set to 50 for the experiments described here, but some simple sentences actually received fewer than 50 parses so n is actually a function of s. Each yield or terminal string in the training, development and test data sets is mapped to such an n-best list of parseprobability pairs; the cross-validation scheme described in Collins 2000 was used to avoid training the n-best parser on the sentence it was being used to parse. A feature extractor, described in section 3, is a vector of m functions f fl, ... , fm, where each fj maps a parse y to a real number fjy, which is the value of the jth feature on y. So a feature extractor maps each y to a vector of feature values fy f1y, ..., fmy. Our reranking parser associates a parse with a score v\\u03b8y, which is a linear function of the feature values fy. That is, each feature fj is associated with a weight \\u03b8j, and the feature values and weights define the score v\\u03b8y of each parse y as follows Given a string s, the reranking parsers output \\u02c6ys on string s is the highest scoring parse in the n-best parses Ys for s, i.e., The feature weight vector \\u03b8 is estimated from the labelled training corpus as described in section 4. Because we use labelled training data we know the correct parse y? s for each sentence s in the training data. The correct parse y? s is not always a member of the n-best parsers output Ys, but we can identify the parses Ys in Ys with the highest f-scores. Informally, the estimation procedure finds a weight vector \\u03b8 that maximizes the score v\\u03b8y of the parses y E Ys relative to the scores of the other parses in Ys, for each s in the training data. The major difficulty in n-best parsing, compared to 1-best parsing, is dynamic programming. For example, n-best parsing is straight-forward in best-first search or beam search approaches that do not use dynamic programming to generate more than one parse, one simply allows the search mechanism to create successive versions to ones hearts content. A good example of this is the Roark parser Roark, 2001 which works left-to right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning. At the end one has a beam-widths number of best parses Roark, 2001. The Collins parser Collins, 1997 does use dynamic programming in its search. That is, whenever a constituent with the same history is generated a second time, it is discarded if its probability is lower than the original version. If the opposite is true, then the original is discarded. This is fine if one only wants the first-best, but obviously it does not directly enumerate the n-best parses. However, Collins Collins, 2000; Collins and Koo, in submission has created an nbest version of his parser by turning off dynamic programming see the users guide to Bikels re-implementation of Collins parser, httpwww.cis.upenn.edu dbikelsoftware.htmlstatparser. As with Roarks parser, it is necessary to add a beam-width constraint to make the search tractable. With a beam width of 1000 the parser returns something like a 50-best list Collins, personal communication, but the actual number of parses returned for each sentences varies. However, turning off dynamic programming results in a loss in efficiency. Indeed, Collinss n-best list of parses for section 24 of the Penn tree-bank has some sentences with only a single parse, because the n-best parser could not find any parses. Now there are two known ways to produce n-best parses while retaining the use of dynamic programming the obvious way and the clever way. The clever way is based upon an algorithm developed by Schwartz and Chow 1990. Recall the key insight in the Viterbi algorithm in the optimal parse the parsing decisions at each of the choice points that determine a parse must be optimal, since otherwise one could find a better parse. This insight extends to n-best parsing as follows. Consider the secondbest parse if it is to differ from the best parse, then at least one of its parsing decisions must be suboptimal. In fact, all but one of the parsing decisions in second-best parse must be optimal, and the one suboptimal decision must be the second-best choice at that choice point. Further, the nth-best parse can only involve at most n suboptimal parsing decisions, and all but one of these must be involved in one of the second through the n1th-best parses. Thus the basic idea behind this approach to n-best parsing is to first find the best parse, then find the second-best parse, then the third-best, and so on. The algorithm was originally described for hidden Markov models. Since this first draft of this paper we have become aware of two PCFG implementations of this algorithm Jimenez and Marzal, 2000; Huang and Chang, 2005. The first was tried on relatively small grammars, while the second was implemented on top of the Bikel re-implementation of the Collins v\\u03b8y. parser Bikel, 2004 and achieved oracle results for 50-best parses similar to those we report below. Here, however, we describe how to find n-best parses in a more straight-forward fashion. Rather than storing a single best parse of each edge, one stores n of them. That is, when using dynamic programming, rather than throwing away a candidate if it scores less than the best, one keeps it if it is one of the top n analyses for this edge discovered so far. This is really very straight-forward. The problem is space. Dynamic programming parsing algorithms for PCFGs require Om2 dynamic programming states, where m is the length of the sentence, so an n-best parsing algorithm requires Onm2. However things get much worse when the grammar is bilexicalized. As shown by Eisner Eisner and Satta, 1999 the dynamic programming algorithms for bilexicalized PCFGs require Om3 states, so a n-best parser would require Onm3 states. Things become worse still in a parser like the one described in Charniak 2000 because it conditions on and hence splits the dynamic programming states according to features of the grandparent node in addition to the parent, thus multiplying the number of possible dynamic programming states even more. Thus nobody has implemented this version. There is, however, one particular feature of the Charniak parser that mitigates the space problem it is a coarse-to-fine parser. By coarse-to-fine we mean that it first produces a crude version of the parse using coarse-grained dynamic programming states, and then builds fine-grained analyses by splitting the most promising of coarse-grained states. A prime example of this idea is from Goodman 1997, who describes amethod for producing a simple but crude approximate grammar of a standard context-free grammar. He parses a sentence using the approximate grammar, and the results are used to constrain the search for a parse with the full CFG. He finds that total parsing time is greatly reduced. A somewhat different take on this paradigm is seen in the parser we use in this paper. Here the parser first creates a parse forest based upon a much less complex version of the complete grammar. In particular, it only looks at standard CFG features, the parent and neighbor labels. Because this grammar encodes relatively little state information, its dynamic programming states are relatively coarse and hence there are comparatively few of them, so it can be efficiently parsed using a standard dynamic programming bottom-up CFG parser. However, precisely because this first stage uses a grammar that ignores many important contextual features, the best parse it finds will not, in general, be the best parse according to the finer-grained second-stage grammar, so clearly we do not want to perform best-first parsing with this grammar. Instead, the output of the first stage is a polynomial-sized packed parse forest which records the left and right string positions for each local tree in the parses generated by this grammar. The edges in the packed parse forest are then pruned, to focus attention on the coarsegrained states that are likely to correspond to highprobability fine-grained states. The edges are then pruned according to their marginal probability conditioned on the string s being parsed as follows Here nij,k is a constituent of type i spanning the words from j to k, \\u03b1nij,k is the outside probability of this constituent, and Qnij,k is its inside probability. From parse forest both \\u03b1 and Q can be computed in time proportional to the size of the compact forest. The parser then removes all constituents nij,k whose probability falls below some preset threshold. In the version of this parser available on the web, this threshold is on the order of 104. The unpruned edges are then exhaustively evaluated according to the fine-grained probabilistic model; in effect, each coarse-grained dynamic programming state is split into one or more fine-grained dynamic programming states. As noted above, the fine-grained model conditions on information that is not available in the coarse-grained model. This includes the lexical head of ones parents, the part of speech of this head, the parents and grandparents category labels, etc. The fine-grained states investigated by the parser are constrained to be refinements of the coarse-grained states, which drastically reduces the number of fine-grained states that need to be investigated. It is certainly possible to do dynamic programming parsing directly with the fine-grained grammar, but precisely because the fine-grained grammar conditions on a wide variety of non-local contextual information there would be a very large number of different dynamic programming states, so direct dynamic programming parsing with the fine-grained grammar would be very expensive in terms of time and memory. As the second stage parse evaluates all the remaining constituents in all of the contexts in which they appear e.g., what are the possible grand-parent labels it keeps track of the most probable expansion of the constituent in that context, and at the end is able to start at the root and piece together the overall best parse. Now comes the easy part. To create a 50-best parser we simply change the fine-grained version of 1-best algorithm in accordance with the obvious scheme outlined earlier in this section. The first, coarse-grained, pass is not changed, but the second, fine-grained, pass keeps the n-best possibilities at each dynamic programming state, rather than keeping just first best. When combining two constituents to form a larger constituent, we keep the best 50 of the 2500 possibilities they offer. Naturally, if we keep each 50-best list sorted, we do nothing like 2500 operations. The experimental question is whether, in practice, the coarse-to-fine architecture keeps the number of dynamic programming states sufficiently low that space considerations do not defeat us. The answer seems to be yes. We ran the algorithm on section 24 of the Penn WSJ tree-bank using the default pruning settings mentioned above. Table 1 shows how the number of fine-grained dynamic programming states increases as a function of sentence length for the sentences in section 24 of the Treebank. There are no sentences of length greater than 69 in this section. Columns two to four show the number of sentences in each bucket, their average length, and the average number of fine-grained dynamic programming structures per sentence. The final column gives the value of the function 100L1.5 where L is the average length of sentences in the bucket. Except for bucket 6, which is abnormally low, it seems that this add-hoc function tracks the number of structures quite well. Thus the number of dynamic programming states does not grow as L2, much less as L3. To put the number of these structures per sentence in perspective, consider the size of such structures. Each one must contain a probability, the nonterminal label of the structure, and a vector of pointers to its children an average parent has slightly more than two children. If one were concerned about every byte this could be made quite small. In our implementation probably the biggest factor is the STL overhead on vectors. If we figure we are using, say, 25 bytes per structure, the total space required is only 1.25Mb even for 50,000 dynamic programming states, so it is clearly not worth worrying about the memory required. The resulting n-bests are quite good, as shown in Table 2. The results are for all sentences of section 23 of the WSJ tree-bank of length 100. From the 1-best result we see that the base accuracy of the parser is 89.7.1 2-best and 10-best show dramatic oracle-rate improvements. After that things start to slow down, and we achieve an oracle rate of 0.968 at 50-best. To put this in perspective, Roark Roark, 2001 reports oracle results of 0.941 with the same experimental setup using his parser to return a variable number of parses. For the case cited his parser returns, on average, 70 parses per sentence. Finally, we note that 50-best parsing is only a factor of two or three slower than 1-best. This section describes how each parse y is mapped to a feature vector fy f1y, ... , fmy. Each feature fj is a function that maps a parse to a real number. The first feature f1y log py is the logarithm of the parse probability p according to the n-best parser model. The other features are integer valued; informally, each feature is associated with a particular configuration, and the features value fjy is the number of times that the configuration that fj indicates. For example, the feature feat pizzay counts the number of times that a phrase in y headed by eat has a complement phrase headed by pizza. Features belong to feature schema, which are abstract schema from which specific features are instantiated. For example, the feature feat pizza is an instance of the Heads schema. Feature schema are often parameterized in various ways. For example, the Heads schema is parameterized by the type of heads that the feature schema identifies. Following Grimshaw 1997, we associate each phrase with a lexical head and a function head. For example, the lexical head of an NP is a noun while the functional head of an NP is a determiner, and the lexical head of a VP is a main verb while the functional head of VP is an auxiliary verb. We experimented with various kinds of feature selection, and found that a simple count threshold performs as well as any of the methods we tried. Specifically, we ignored all features that did not vary on the parses of at least t sentences, where t is the count threshold. In the experiments described below t 5, though we also experimented with t 2. The rest of this section outlines the feature schemata used in the experiments below. These feature schemata used here were developed using the n-best parses provided to us by Michael Collins approximately a year before the n-best parser described here was developed. We used the division into preliminary training and preliminary development data sets described in Collins 2000 while experimenting with feature schemata; i.e., the first 36,000 sentences of sections 220 were used as preliminary training data, and the remaining sentences of sections 20 and 21 were used as preliminary development data. It is worth noting that developing feature schemata is much more of an art than a science, as adding or deleting a single schema usually does not have a significant effect on performance, yet the overall impact of many well-chosen schemata can be dramatic. Using the 50-best parser output described here, there are 1,148,697 features that meet the count threshold of at least 5 on the main training data i.e., Penn treebank sections 221. We list each feature schemas name, followed by the number of features in that schema with a count of at least 5, together with a brief description of the instances of the schema and the schemas parameters. CoPar 10 The instances of this schema indicate conjunct parallelism at various different depths. For example, conjuncts which have the same label are parallel at depth 0, conjuncts with the same label and whose children have the same label are parallel at depth 1, etc. CoLenPar 22 The instances of this schema indicate the binned difference in length in terms of number of preterminals dominated in adjacent conjuncts in the same coordinated structures, conjoined with a boolean flag that indicates whether the pair is final in the coordinated phrase. RightBranch 2 This schema enables the reranker to prefer right-branching trees. One instance of this schema returns the number of nonterminal nodes that lie on the path from the root node to the right-most non-punctuation preterminal node, and the other instance of this schema counts the number of the other nonterminal nodes in the parse tree. Heavy 1049 This schema classifies nodes by their category, their binned length i.e., the number of preterminals they dominate, whether they are at the end of the sentence and whether they are followed by punctuation. Neighbours 38,245 This schema classifies nodes by their category, their binned length, and the part of speech categories of the E1 preterminals to the nodes left and the 2 preterminals to the nodes right. E1 and E2 are parameters of this schema; here E1 1 or E1 2 and E2 1. Rule 271,655 The instances of this schema are local trees, annotated with varying amounts of contextual information controlled by the schemas parameters. This schema was inspired by a similar schema in Collins and Koo in submission. The parameters to this schema control whether nodes are annotated with their preterminal heads, their terminal heads and their ancestors categories. An additional parameter controls whether the feature is specialized to embedded or non-embedded clauses, which roughly corresponds to Emonds nonroot and root contexts Emonds, 1976. NGram 54,567 The instances of this schema are E-tuples of adjacent children nodes of the same parent. This schema was inspired by a similar schema in Collins and Koo in submission. This schema has the same parameters as the Rule schema, plus the length E of the tuples of children E 2 here. Heads 208,599 The instances of this schema are tuples of head-to-head dependencies, as mentioned above. The category of the node that is the least common ancestor of the head and the dependent is included in the instance this provides a crude distinction between different classes of arguments. The parameters of this schema are whether the heads involved are lexical or functional heads, the number of heads in an instance, and whether the lexical item or just the heads part of speech are included in the instance. LezFunHeads 2,299 The instances of this feature are the pairs of parts of speech of the lexical head and the functional head of nodes in parse trees. WProj 158,771 The instances of this schema are preterminals together with the categories of E of their closest maximal projection ancestors. The parameters of this schema control the number E of maximal projections, and whether the preterminals and the ancestors are lexicalized. Word 49,097 The instances of this schema are lexical items together with the categories of E of their immediate ancestor nodes, where E is a schema parameter E 2 or E 3 here. This feature was inspired by a similar feature in Klein and Manning 2003. HeadTree 72,171 The instances of this schema are tree fragments consisting of the local trees consisting of the projections of a preterminal node and the siblings of such projections. This schema is parameterized by the head type lexical or functional used to determine the projections of a preterminal, and whether the head preterminal is lexicalized. NGramTree 291,909 The instances of this schema are subtrees rooted in the least common ancestor of E contiguous preterminal nodes. This schema is parameterized by the number E of contiguous preterminals E 2 or E 3 here and whether these preterminals are lexicalized. This section explains how we estimate the feature weights 0 01, ... , 0m for the feature functions f f1, ... , fm. We use a MaxEnt estimator to find the feature weights \\u02c60, where L is the loss function and R is a regularization penalty term The training data D s1, ... , sn, is a sequence of sentences and their correct parses y? s1, ... , y?sn. We used the 20-fold crossvalidation technique described in Collins 2000 to compute the n-best parses Ys for each sentence s in D. In general the correct parse y? s is not a member of Ys, so instead we train the reranker to identify one of the best parses Ys arg maxyYs Fy,,sy in the n-best parsers output, where Fy,,y is the Parseval f-score of y evaluated with respect to y?. Because there may not be a unique best parse for each sentence i.e., Ys 1 for some sentences s we used the variant of MaxEnt described in Riezler et al. 2002 for partially labelled training data. Recall the standard MaxEnt conditional probability model for a parse y E Y The loss function LD proposed in Riezler et al. 2002 is just the negative log conditional likelihood of the best parses Ys relative to the n-best parser output Ys The partial derivatives of this loss function, which are required by the numerical estimation procedure, are In the experiments reported here, we used a Gaussian or quadratic regularizer Rw cPmj1 w2j, where c is an adjustable parameter that controls the amount of regularization, chosen to optimize the rerankers f-score on the development set section 24 of the treebank. We used the Limited Memory Variable Metric optimization algorithm from the PETScTAO optimization toolkit Benson et al., 2004 to find the optimal feature weights \\u03b8\\u02c6 because this method seems substantially faster than comparable methods Malouf, 2002. The PETScTAO toolkit provides a variety of other optimization algorithms and flags for controlling convergence, but preliminary experiments on the Collins trees with different algorithms and early stopping did not show any performance improvements, so we used the default PETScTAO setting for our experiments here. We evaluated the performance of our reranking parser using the standard PARSEVAL metrics. We n-best trees f-score New 0.9102 Collins 0.9037 best trees, with weights estimated from sections 2 21 and the regularizer constant c adjusted for optimal f-score on section 24 and evaluated on sentences of length less than 100 in section 23. trained the n-best parser on sections 221 of the Penn Treebank, and used section 24 as development data to tune the mixing parameters of the smoothing model. Similarly, we trained the feature weights \\u03b8 with the MaxEnt reranker on sections 221, and adjusted the regularizer constant c to maximize the f-score on section 24 of the treebank. We did this both on the trees supplied to us by Michael Collins, and on the output of the n-best parser described in this paper. The results are presented in Table 3. The n-best parsers most probable parses are already of state-of-the-art quality, but the reranker further improves the f-score. This paper has described a dynamic programming n-best parsing algorithm that utilizes a heuristic coarse-to-fine refinement of parses. Because the coarse-to-fine approach prunes the set of possible parse edges beforehand, a simple approach which enumerates the n-best analyses of each parse edge is not only practical but quite efficient. We use the 50-best parses produced by this algorithm as input to a MaxEnt discriminative reranker. The reranker selects the best parse from this set of parses using a wide variety of features. The system we described here has an f-score of 0.91 when trained and tested using the standard PARSEVAL framework. This result is only slightly higher than the highest reported result for this test-set, Bods .907 Bod, 2003. More to the point, however, is that the system we describe is reasonably efficient so it can be used for the kind of routine parsing currently being handled by the Charniak or Collins parsers. A 91.0 f-score represents a 13 reduction in fmeasure error over the best of these parsers.2 Both the 50-best parser, and the reranking parser can be found at ftpftp.cs.brown.edupubnlparser, named parser and reranker respectively. Acknowledgements We would like to thanks Michael Collins for the use of his data and many helpful comments, and Liang Huang for providing an early draft of his paper and very useful comments on our paper. Finally thanks to the National Science Foundation for its support NSF IIS-0112432, NSF 9721276, and NSF DMS-0074276.\",\n          \"Scaling Web-Based Acquisition Of Entailment Relations Paraphrase recognition is a critical step for natural language interpretation. Accordingly, many NLP applications would benefit from high coverage knowledge bases of paraphrases. However, the scalability of state-of-the-art paraphrase acquisition approaches is still limited. We present a fully unsupervised learning algorithm for Web-based extraction an extended model of paraphrases. We focus on increased scalability and generality with respect to prior work, eventually aiming at a full scale knowledge base. Our current implementation of the algorithm takes as its input a verb lexicon and for each verb searches the Web for related syntactic entailment templates. Experiments show promising results with respect to the ultimate goal, achieving much better scalability than prior Web-based methods. Modeling semantic variability in language has drawn a lot of attention in recent years. Many applications like QA, IR, IE and Machine Translation Moldovan and Rus, 2001; Hermjakob et al., 2003; Jacquemin, 1999 have to recognize that the same meaning can be expressed in the text in a huge variety of surface forms. Substantial research has been dedicated to acquiring paraphrase patterns, which represent various forms in which a certain meaning can be expressed. Following Dagan and Glickman, 2004 we observe that a somewhat more general notion needed for applications is that of entailment relations e.g. Moldovan and Rus, 2001. These are directional relations between two expressions, where the meaning of one can be entailed from the meaning of the other. For example X acquired Y entails X owns Y. These relations provide a broad framework for representing and recognizing semantic variability, as proposed in Dagan and Glickman, 2004. For example, if a QA system has to answer the question Who owns Overture? and the corpus includes the phrase Yahoo acquired Overture, the system can use the known entailment relation to conclude that this phrase really indicates the desired answer. More examples of entailment relations, acquired by our method, can be found in Table 1 section 4. To perform such inferences at a broad scale, applications need to possess a large knowledge base KB of entailment patterns. We estimate such a KB should contain from between a handful to a few dozens of relations per meaning, which may sum to a few hundred thousands of relations for a broad domain, given that a typical lexicon includes tens of thousands of words. Our research goal is to approach unsupervised acquisition of such a full scale KB. We focus on developing methods that acquire entailment relations from the Web, the largest available resource. To this end substantial improvements are needed in order to promote scalability relative to current Webbased approaches. In particular, we address two major goals reducing dramatically the complexity of required auxiliary inputs, thus enabling to apply the methods at larger scales, and generalizing the types of structures that can be acquired. The algorithms described in this paper were applied for acquiring entailment relations for verb-based expressions. They successfully discovered several relations on average per each randomly selected expression. This section provides a qualitative view of prior work, emphasizing the perspective of aiming at a full-scale paraphrase resource. As there are still no standard benchmarks, current quantitative results are not comparable in a consistent way. The major idea in paraphrase acquisition is often to find linguistic structures, here termed templates, that share the same anchors. Anchors are lexical elements describing the context of a sentence. Templates that are extracted from different sentences and connect the same anchors in these sentences, are assumed to paraphrase each other. For example, the sentences Yahoo bought Overture and Yahoo acquired Overture share the anchors XYahoo, YOverture, suggesting that the templates X buy Y and X acquire Y paraphrase each other. Algorithms for paraphrase acquisition address two problems a finding matching anchors and b identifying template structure, as reviewed in the next two subsections. The prominent approach for paraphrase learning searches sentences that share common sets of multiple anchors, assuming they describe roughly the same fact or event. To facilitate finding many matching sentences, highly redundant comparable corpora have been used. These include multiple translations of the same text Barzilay and McKeown, 2001 and corresponding articles from multiple news sources Shinyama et al., 2002; Pang et al., 2003; Barzilay and Lee, 2003. While facilitating accuracy, we assume that comparable corpora cannot be a sole resource due to their limited availability. Avoiding a comparable corpus, Glickman and Dagan, 2003 developed statistical methods that match verb paraphrases within a regular corpus. Their limited scale results, obtaining several hundred verb paraphrases from a 15 million word corpus, suggest that much larger corpora are required. Naturally, the largest available corpus is the Web. Since exhaustive processing of the Web is not feasible, Duclaye et al., 2002 and Ravichandran and Hovy, 2002 attempted bootstrapping approaches, which resemble the mutual bootstrapping method for Information Extraction of Riloff and Jones, 1999. These methods start with a provided known set of anchors for a target meaning. For example, the known anchor set Mozart, 1756 is given as input in order to find paraphrases for the template X born in Y. Web searching is then used to find occurrences of the input anchor set, resulting in new templates that are supposed to specify the same relation as the original one born in. These new templates are then exploited to get new anchor sets, which are subsequently processed as the initial Mozart, 1756. Eventually, the overall procedure results in an iterative process able to induce templates from anchor sets and vice versa. The limitation of this approach is the requirement for one input anchor set per target meaning. Preparing such input for all possible meanings in broad domains would be a huge task. As will be explained below, our method avoids this limitation by finding all anchor sets automatically in an unsupervised manner. Finally, Lin and Pantel, 2001 present a notably different approach that relies on matching separately single anchors. They limit the allowed structure of templates only to paths in dependency parses connecting two anchors. The algorithm constructs for each possible template two feature vectors, representing its co-occurrence statistics with the two anchors. Two templates with similar vectors are suggested as paraphrases termed inference rule. Matching of single anchors relies on the general distributional similarity principle and unlike the other methods does not require redundancy of sets of multiple anchors. Consequently, a much larger number of paraphrases can be found in a regular corpus. Lin and Pantel report experiments for 9 templates, in which their system extracted 10 correct inference rules on average per input template, from 1GB of news data. Yet, this method also suffers from certain limitations a it identifies only templates with pre-specified structures; b accuracy seems more limited, due to the weaker notion of similarity; and c coverage is limited to the scope of an available corpus. To conclude, several approaches exhaustively process different types of corpora, obtaining varying scales of output. On the other hand, the Web is a huge promising resource, but current Web-based methods suffer serious scalability constraints. Paraphrasing approaches learn different kinds of template structures. Interesting algorithms are presented in Pang et al., 2003; Barzilay and Lee, 2003. They learn linear patterns within similar contexts represented as finite state automata. Three classes of syntactic template learning approaches are presented in the literature learning ofpredicate argument templates Yangarber et al., 2000, learning of syntactic chains Lin and Pantel, 2001 and learning of sub-trees Sudo et al., 2003. The last approach is the most general with respect to the template form. However, its processing time increases exponentially with the size of the templates. As a conclusion, state of the art approaches still learn templates of limited form and size, thus restricting generality of the learning process. Motivated by prior experience, we identify two major goals for scaling Web-based acquisition of entailment relations a Covering the broadest possible range of meanings, while requiring minimal input and b Keeping template structures as general as possible. To address the first goal we require as input only a phrasal lexicon of the relevant domain including single words and multiword expressions. Broad coverage lexicons are widely available or may be constructed using known term acquisition techniques, making it a feasible and scalable input requirement. We then aim to acquire entailment relations that include any of the lexicons entries. The second goal is addressed by a novel algorithm for extracting the most general templates being justified by the data. For each lexicon entry, denoted a pivot, our extraction method performs two phases a extract promising anchor sets for that pivot ASE, Section 3.1, and b from sentences containing the anchor sets, extract templates for which an entailment relation holds with the pivot TE, Section 3.2. Examples for verb pivots are acquire, fall to, prevent. We will use the pivot prevent for examples through this section. Before presenting the acquisition method we first define its output. A template is a dependency parsetree fragment, with variable slots at some tree nodes e.g. X s_ prevent Y. An entailment relation between two templates T1 and T2 holds if the meaning of T2 can be inferred from the meaning of T1 or vice versa in some contexts, but not necessarily all, under the same variable instantiation. For example, X s prevent 0 Y entails X s_ reduce - Y risk because the sentence aspirin reduces heart attack risk can be inferred from aspirin prevents a first heart attack. Our output consists of pairs of templates for which an entailment relation holds. The goal of this phase is to find a substantial number of promising anchor sets for each pivot. A good anchor-set should satisfy a proper balance between specificity and generality. On one hand, an anchor set should correspond to a sufficiently specific setting, so that entailment would hold between its different occurrences. On the other hand, it should be sufficiently frequent to appear with different entailing templates. Finding good anchor sets based on just the input pivot is a hard task. Most methods identify good repeated anchors in retrospect, that is after processing a full corpus, while previous Web-based methods require at least one good anchor set as input. Given our minimal input, we needed refined criteria that identify a priori the relatively few promising anchor sets within a sample of pivot occurrences. The ASE algorithm presented in Figure 1 performs 4 main steps. STEP 1 creates a complete template, called the pivot template and denoted Tp, for the input pivot, denoted P. Variable slots are added for the major types of syntactic relations that interact with P, based on its syntactic type. These slots enable us to later match Tp with other templates. For verbs, we add slots for a subject and for an object or a modifier e.g. X s_ prevent Y. STEP 2 constructs asample corpus, denoted S, for the pivot template. STEP 2.A utilizes a Web search engine to initialize S by retrieving sentences containing P. The sentences are parsed by the MINIPAR dependency parser Lin, 1998, keeping only sentences that contain the complete syntactic template Tp with all the variables instantiated. STEP 2.B identifies phrases that are statistically associated with Tp in S. We test all noun-phrases in S , discarding phrases that are too common on the Web absolute frequency higher than a threshold MAXPHRASEF, such as desire. Then we select the N phrases with highest tf idf score1. These phrases have a strong collocation relationship with the pivot P and are likely to indicate topical rather than anecdotal occurrences of P. For example, the phrases patient and American Dental Association, which indicate contexts of preventing health problems, were selected for the pivot prevent. Fi1Here, tf idf freqSX log freqN X where freqSX is the number of occurrences in S containing X, N is the total number of Web documents, and freqW X is the number of Web documents containing X. nally, STEP 2.C expands S by querying the Web with the both P and each of the associated phrases, adding the retrieved sentences to S as in step 2.a. STEP 3 extracts candidate anchor sets for Tp. From each sentence in S we try to generate one candidate set, containing noun phrases whose Web frequency is lower than MAXPHRASEF. STEP 3.A extracts slot anchors phrases that instantiate the slot variables of Tp. Each anchor is marked with the corresponding slot. For example, the anchors antibioticssubj , miscarriage obj were extracted from the sentence antibiotics in pregnancy prevent miscarriage. STEP 3.B tries to extend each candidate set with one additional context anchor, in order to improve its specificity. This anchor is chosen as the highest tf idf scoring phrase in the sentence, if it exists. In the previous example, pregnancy is selected. STEP 4 filters out bad candidate anchor sets by two different criteria. STEP 4.A maintains only candidates with absolute Web frequency within a threshold range MINSETF, MAXSETF, to guarantee an appropriate specificity-generality level. STEP 4.B guarantees sufficient directional association between the candidate anchor set c and Tp, by estimating where freqW is Web frequency and P is the pivot. We maintain only candidates for which this probability falls within a threshold range SETMINP, SETMAXP. Higher probability often corresponds to a strong linguistic collocation between the candidate and Tp, without any semantic entailment. Lower probability indicates coincidental cooccurrence, without a consistent semantic relation. The remaining candidates in S become the input anchor-sets for the template extraction phase, for example, Aspirinsubj , heart attackobj for prevent. The Template Extraction algorithm accepts as its input a list of anchor sets extracted from ASE for each pivot template. Then, TE generates a set of syntactic templates which are supposed to maintain an entailment relationship with the initial pivot template. TE performs three main steps, described in the following subsections For each input anchor set, TE acquires from the Web a sample corpus of sentences containing it. For example, a sentence from the sample corpus for aspirin, heart attack is Aspirin stops heart attack?. All of the sample sentences are then parsed with MINIPAR Lin, 1998, which generates from each sentence a syntactic directed acyclic graph DAG representing the dependency structure of the sentence. Each vertex in this graph is labeled with a word and some morphological information; each graph edge is labeled with the syntactic relation between the words it connects. TE then substitutes each slot anchor see section 3.1 in the parse graphs with its corresponding slot variable. Therefore, Aspirin stops heart attack? will be transformed into X stop Y. This way all the anchors for a certain slot are unified under the same variable name in all sentences. The parsed sentences related to all of the anchor sets are subsequently merged into a single set of parse graphs S P1, P2, ... , Pn see P1 and P2 in Figure 2. The core of TE is a General Structure Learning algorithm GSL that is applied to the set of parse graphs S resulting from the previous step. GSL extracts single-rooted syntactic DAGs, which are named spanning templates since they must span at least over Na slot variables, and should also appear in at least Nr sentences from S In our experiments we set Na2 and Nr2. GSL learns maximal most general templates they are spanning templates which, at the same time, a cannot be generalized by further reduction and b cannot be further extended keeping the same generality level. In order to properly define the notion of maximal most general templates, we introduce some formal definitions and notations. DEFINITION For a spanning template t we define a sentence set, denoted with \\u03c3t, as the set of all parsed sentences in S containing t. For each pair of templates t1 and t2, we use the notation t1 t2 to denote that t1 is included as a subgraph or is equal to t2. We use the notation t1 t2 when such inclusion holds strictly. We define TS as the set of all spanning templates in the sample S. DEFINITION A spanning template t E T S is maximal most general if and only if both of the following conditions hold Condition A ensures that the extracted templates do not contain spanning sub-structures that are more general i.e. having a larger sentence set; condition B ensures that the template cannot be further enlarged without reducing its sentence set. GSL performs template extraction in two main steps 1 build a compact graph representation of all the parse graphs from S; 2 extract templates from the compact representation. A compact graph representation is an aggregate graph which joins all the sentence graphs from S ensuring that all identical spanning sub-structures from different sentences are merged into a single one. Therefore, each vertex v respectively, edge e in the aggregate graph is either a copy of a corresponding vertex edge from a sentence graph Pi or it represents the merging of several identically labeled vertices edges from different sentences in S. The set of such sentences is defined as the sentence set of v e, and is represented through the set of index numbers of related sentences e.g. 1,2 in the third tree of Figure 2. We will denote with Gi the compact graph representation of the first i sentences in S. The parse trees P1 and P2 of two sentences and their related compact representation G2 are shown in Figure 2. Building the compact graph representation The compact graph representation is built incrementally. The algorithm starts with an empty aggregate graph G0 and then merges the sentence graphs from S one at a time into the aggregate structure. Lets denote the current aggregate graph with Gi_1Vg, Eg and let PiVp, Ep be the parse graph which will be merged next. Note that the sentence set of Pi is a single element set W. During each iteration a new graph is created as the union of both input graphs Gi Gi_1 U Pi. Then, the following merging procedure is performed on the elements of Gi ated and added to Gi. The new vertex takes the same label and holds a sentence set which is formed from the sentence set of vg by adding i to it. Still with reference to Figure 2, the generalized vertices in G2 are X, Y and stop. The algorithm connects the generalized vertex vnew g with all the vertices which are connected with vg and vp. As an optimization step, we merge only vertices and edges that are included in equal spanning templates. Extracting the templates GSL extracts all maximal most general templates from the final compact representation Gn using the following sub-algorithm In Figure 2 the maximal most general template in obj As a last step, names and numbers are filtered out from the templates. Moreover, TE removes those templates which are very long or which appear with just one anchor set and in less than four sentences. Finally, the templates are sorted first by the number of anchor sets with which each template appeared, and then by the number of sentences in which they appeared. We evaluated the results of the TEASE algorithm on a random lexicon of verbal forms and then assessed its performance on the extracted data through human-based judgments. The test set for human evaluation was generated by picking out 53 random verbs from the 1000 most frequent ones found in a subset of the Reuters corpus2. For each verb entry in the lexicon, we provided the judges with the corresponding pivot template and the list of related candidate entailment templates found by the system. The judges were asked to evaluate entailment for a total of 752 templates, extracted for 53 pivot lexicon entries; Table 1 shows a sample of the evaluated templates; all of them are clearly good and were judged as correct ones. included in the evaluation test set. Concerning the ASE algorithm, threshold parameters3 were set as PHRASEMAXF107, SETMINF102, SETMAXF105, SETMINP0.066, and SETMAXP0.666. An upper limit of 30 was imposed on the number of possible anchor sets used for each pivot. Since this last value turned out to be very conservative with respect to system coverage, we subsequently attempted to relax it to 50 see Discussion in Section 4.3. Further post-processing was necessary over extracted data in order to remove syntactic variations referring to the same candidate template typically passiveactive variations. Three possible judgment categories have been considered Correct if an entailment relationship in at least one direction holds between the judged template and the pivot template in some non-bizarre context; Incorrect if there is no reasonable context and variable instantiation in which entailment holds; No Evaluation if the judge cannot come to a definite conclusion. Each of the three assessors referred to as J1, J2, and J3 issued judgments for the 752 different templates. Correct templates resulted to be 283, 313, and 295 with respect to the three judges. No evaluations were 2, 0, and 16, while the remaining templates were judged Incorrect. For each verb, we calculate Yield as the absolute number of Correct templates found and Precision as the percentage of good templates out of all extracted templates. Obtained Precision is 44.15, averaged over the 53 verbs and the 3 judges. Considering Low Majority on judges, the precision value is 42.39. Average Yield was 5.5 templates per verb. These figures may be compared informally, as data is incomparable with average yield of 10.1 and average precision of 50.3 for the 9 pivot templates of Lin and Pantel, 2001. The comparison suggests that it is possible to obtain from the very noisy web a similar range of precision as was obtained from a clean news corpus. It also indicates that there is potential for acquiring additional templates per pivot, which would require further research on broadening efficiently the search for additional web data per pivot. Agreement among judges is measured by the Kappa value, which is 0.55 between J1 and J2, 0.57 between J2 and J3, and 0.63 between J1 and J3. Such Kappa values correspond to moderate agreement for the first two pairs and substantial agreement for the third one. In general, unanimous agreement among all of the three judges has been reported on 519 out of 752 templates, which corresponds to 69. Our algorithm obtained encouraging results, extracting a considerable amount of interesting templates and showing inherent capability of discovering complex semantic relations. Concerning overall coverage, we managed to find correct templates for 86 of the verbs 46 out of 53. Nonetheless, presented results show a substantial margin of possible improvement. In fact yield values 5.5 Low Majority, up to 24 in best cases, which are our first concern, are inherently dependent on the breadth of Web search performed by the ASE algorithm. Due to computational time, the maximal number of anchor sets processed for each verb was held back to 30, significantly reducing the amount of retrieved data. In order to further investigate ASE potential, we subsequently performed some extended experiment trials raising the number of anchor sets per pivot to 50. This time we randomly chose a subset of 10 verbs out of the less frequent ones in the original main experiment. Results for these verbs in the main experiment were an average Yield of 3 and an average Precision of 45.19. In contrast, the extended experiments on these verbs achieved a 6.5 Yield and 59.95 Precision average values. These results are indeed promising, and the substantial growth in Yield clearly indicates that the TEASE algorithms can be further improved. We thus suggest that the feasibility of our approach displays the inherent scalability of the TEASE process, and its potential to acquire a large entailment relation KB using a full scale lexicon. A further improvement direction relates to template ranking and filtering. While in this paper we considered anchor sets to have equal weights, we are also carrying out experiments with weights based on cross-correlation between anchor sets. We have described a scalable Web-based approach for entailment relation acquisition which requires only a standard phrasal lexicon as input. This minimal level of input is much simpler than required by earlier web-based approaches, while succeeding to maintain good performance. This result shows that it is possible to identify useful anchor sets in a fully unsupervised manner. The acquired templates demonstrate a broad range of semantic relations varying from synonymy to more complicated entailment. These templates go beyond trivial paraphrases, demonstrating the generality and viability of the presented approach. From our current experiments we can expect to learn about 5 relations per lexicon entry, at least for the more frequent entries. Moreover, looking at the extended test, we can extrapolate a notably larger yield by broadening the search space. Together with the fact that we expect to find entailment relations for about 85 of a lexicon, it is a significant step towards scalability, indicating that we will be able to extract a large scale KB for a large scale lexicon. In future work we aim to improve the yield by increasing the size of the sample-corpus in a qualitative way, as well as precision, using statistical methods such as supervised learning for better anchor set identification and cross-correlation between different pivots. We also plan to support noun phrases as input, in addition to verb phrases. Finally, we would like to extend the learning task to discover the correct entailment direction between acquired templates, completing the knowledge required by practical applications. Like Lin and Pantel, 2001, learning the context for which entailment relations are valid is beyond the scope of this paper. As stated, we learn entailment relations holding for some, but not necessarily all, contexts. In future work we also plan to find the valid contexts for entailment relations. The authors would like to thank Oren Glickman Bar Ilan University for helpful discussions and assistance in the evaluation, Bernardo Magnini for his scientific supervision at ITC-irst, Alessandro Vallin and Danilo Giampiccolo ITC-irst for their help in developing the human based evaluation, and Prof. Yossi Matias Tel-Aviv University for supervising the first author. This work was partially supported by the MOREWEB project, financed by Provincia Autonoma di Trento. It was also partly carried out within the framework of the ITC-IRST TRENTO, ITALY UNIVERSITY OF HAIFA ISRAEL collaboration project. For data visualization and analysis the authors intensively used the CLARK system www.bultreebank.org developed at the Bulgarian Academy of Sciences.\",\n          \"Exploiting Diverse Knowledge Sources Via Maximum Entropy In Named Entity Recognition This paper describes a novel statistical namedentity i.e. quot;proper namequot; recognition system built around a maximum entity framework. By working within the framework of maximum entropy theory and utilizing a flexible object-based architecture, the system is able to make use of an extraordinarily diverse range of knowledge sources in making its tagging decisions. These knowledge sources include capitalization features, lexical features, features indicating the current section of text i.e. headline or main body, and dictionaries of single or multi-word terms. The purely statistical system contains no hand-generated patterns and achieves a result comparable with the best statistical systems. However, when combined with other handcoded systems, the system achieves scores that exceed the highest comparable scores thus-far published. Named entity recognition is one of the simplest of the common message understanding tasks. The objective is to identify and categorize all members of certain categories of quot;proper namesquot; from a given corpus. The specific test bed which will be the subject of this paper is that of the Seventh Message Understanding Conference MUC-7, in which the task was to identify quot;namesquot; falling into one of seven categories person, organization, location, date, time, percentage, and monetary amount. This paper describes a new system called quot;Maximum Entropy Named Entityquot; or quot;MENEquot; pronounced quot;meaniequot;. By working within the framework of maximum entropy theory and utilizing a flexible object-based architecture, the system is able to make use of an extraordinarily diverse range of knowledge sources in making its tagging decision. These knowledge sources include capitalization features, lexical features, and features indicating the current section of text. It makes use of a broad array of dictionaries of useful single or multi-word terms such as first names, company names, and corporate suffixes, and automatically handles cases where words are in more than one dictionary. Our dictionaries required no manual editing and were either downloaded from the web or were simply quot;obviousquot; lists entered by hand. This system, built from off-the-shelf knowledge sources, contained no hand-generated patterns and achieved a result which is comparable with that of the best statistical systems. Further experiments showed that when combined with handcoded systems from NYU, the University of Manitoba, and IsoQuest, Inc., MENE was able to generate scores which exceeded the highest scores thus-far reported by any system on a MUC evaluation. Given appropriate training data, we believe that this system is highly portable to other domains and languages and have already achieved good results on upper-case English. We also feel that there are plenty of avenues to explore in enhancing the system's performance on English-language newspaper text. Given a tokenization of a test corpus and a set of n for MUC-7, n 7 tags which define the name categories of the task at hand, the problem of named entity recognition can be reduced to the problem of assigning one of 4n 1 tags to each token. For any particular tag x from the set of n tags, we could be in one of 4 states x_start, x_continue, x_end, and x_unique. In addition, a token could be tagged as quot;otherquot; to indicate that it is not part of a named entity. For instance, we would tag the phrase Jerry Lee Lewis flew to Paris as person_start, person_continue, person_end, other, other, location_unique. This approach is essentially the same as Sekine et al., 1998. The 29 tags of MUC-7 form the space of quot;futuresquot; for a maximum entropy formulation of our N.E. problem. A maximum entropy solution to this, or any other similar problem allows the computation of pflh for any f from the space of possible futures, F, for every h from the space of possible histories, H. A quot;historyquot; in maximum entropy is all of the conditioning data which enables you to make a decision among the space of futures. In the named entity problem, we could reformulate this in terms of finding the probability of f associated with the token at index tin the test corpus as Pf Iht P f Itest corpus relative to token t Information derivable from the The computation of pf1h in M.E. is dependent on a set of quot;featuresquot; which, hopefully, are helpful in making a prediction about the future. Like most current M.E. modeling efforts in computational linguistics we restrict ourselves to features which are binary functions of the history and future. For instance, one of our features is Here quot;current-token-capitalizedhquot; is a binary function which returns true if the quot;current tokenquot; of the history h the token whose tag we are trying to determine has an initial capitalized letter. Given a set of features and some training data, the maximum entropy estimation process produces a model in which every feature gi has associated with it a parameter ai. This allows us to compute the conditional probability as follows Berger et al., 1996 The maximum entropy estimation technique guarantees that for every feature gi, the expected value of gi according to the M.E. model will equal the empirical expectation of gi in the training corpus. In other words Here P is an empirical probability and PmE is the probability assigned by the M.E. model. More complete discussions of M.E. as applied to computational linguistics, including a description of the M.E. estimation procedure can be found in Berger et al., 1996 and Della Pietra et al., 1995. The following are some additional references which are useful as introductions and examples of applications Ratnaparkhi, 1997b Ristad, 1.998 Jaynes, 1996. As many authors have remarked, though, the most useful thing about maximum entropy modeling is that it allows the modeler to concentrate on finding the features that characterize the problem while letting the M.E. estimation routine worry about assigning the relative weights to the features. MENE consists of a set of C and Pen l modules which forms a wrapper around a publicly available M.E. toolkit Ristad, 1998 which computes the values of the a parameters of equation 2 from a pair of training files created by MENE. MENE's flexibility is due to its object-based treatment of the three essential components of a maximum entropy system histories, futures, and features Borthwick et al., 1997. History objects in MENE act as containers for a list of quot;history viewsquot;. The history view classes each represent a different type of information about the history object. When the features attempt to determine whether or not they fire on a given history, they request an appropriate history view object from the history object and then query the history view object to determine whether their firing conditions are satisfied. Note that these history views generally hold information about a limited window around the current token. If the current token is denoted as wo, then our model only holds information about tokens w1 for all history views except the lexical ones. For these views, the window is w-2 W. Future objects, on the other hand, are trivial in that their only piece of data is an integer indicating which of the 29 members of the future space they represent. Features are implemented as binary valued functions which query the history and future objects to determine whether or not they quot;firequot;. In the following sections, we will look at each of MENE's feature classes in turn. While all of MENE's features have binary-valued output, the quot;binaryquot; features are features whose associated history-view can be considered to be either on or off for a given token. Examples are quot;the token begins with a capitalized letterquot; or quot;the token is a four-digit numberquot;. Equation I gives an example of a binary feature. The 11 binary history-views used by MENE's binary features are very similar to those used in BBN's NymbleIdentifinder system Bikel et al., 1997 with two exceptions b, when the history,future space on which feature b activates must be a subset of the space for feature a, it can be shown that the M.E. model will yield the same results whether a and b are included as features or if a b and b are features. Consequently, MENE allows all features to fire in overlapping cases. For instance, in MENE the initial cap features activate on the histories quot;Clintonquot;, quot;IBMquot;, and quot;ValuJetquot; while in Nymble the feature would only be active on quot;Clintonquot; because the quot;AllCapquot; feature would take precedence on quot;IBMquot; and an quot;Initial-and-internal-capquot; feature would take precedence on quot;ValuJetquot;. To create a lexical history view, the tokens at w-2 w2 are compared with a vocabulary and their vocabulary indices are recorded. For a given training corpus. we define the vocabulary to be all tokens with a count of three or more. Words not found in the vocabulary are assigned a distinguished quot;Unknownquot; index. Lexical feature example A more subtle feature picked up by MENE preceding word is quot;toquot; and future is quot;location_uniquequot;. Given the domain of the MUC-7 training data aviation disasters, quot;toquot; is a weak indicator, but a real one. This is an example of a feature which MENE can make use of but which the constructor of a handcoded system would probably regard as too risky to incorporate. This feature, in conjunction with other weak features, can allow MENE to pick up names that other systems might miss. As discussed later, these features are automatically acquired and the system can attain a very high level of performance using these features alone. This is encouraging since these lexical features are not dependent on any external knowledge source or linguistic intuition and thus are completely portable to new domains. The New York Times articles which constituted the MUC-7 test and training corpora were composed of six distinct sections including quot;Datequot;, quot;Preamblequot;, and quot;Textquot;. Section features activate according to which of these sections the current token is in. Example feature if Section-Viewtokenoh .9h, n 1 quot;Preamblequot; and f person_unique 0 else Activation example CLINTON WARNS HUSSEIN ABOUT IRAQI DEFIANCE. Note that, assuming that this headline is in the preamble, the above feature will fire on all of these words. Of course, this feature's prediction will only be correct on quot;CLINTONquot; and quot;HUSSEINquot;. Section features establish the background probability of the occurrence of the different futures. For instance, in NYU's evaluation system, the a value assigned to the feature which predicts quot;otherquot; given a current section of quot;main body of textquot; is 7.9 times stronger than the feature which predicts quot;person_uniquequot; in the same section. Thus the system predicts quot;otherquot; by default. On the other hand, in the preamble which contains headline, author, etc. information, the feature predicting quot;otherquot; is much weaker in most cases. It is only about 2.6 times as strong as quot;organization_startquot; and quot;organization_endquot;, for instance. Multi-word dictionaries are a key element of MENE. Each entry in a MENE dictionary consists of a term which is one or more tokens long. Dictionaries can be case-sensitive or not on a dictionary-bydictionary basis. A pre-processing step summarizes the information in the dictionary on a token-bytoken basis by assigning to every token one of the following five tags for each dictionary start, continue, end, unique, other. I.e. if quot;British Airwaysquot; was in our dictionary, a dictionary feature would see the phrase quot;on British Airways Flight 962quot; as quot;other, start, end, other, otherquot;. Table 1 lists the dictionaries. used by MENE in the MUC-7 evaluation. Below is an example of a dictionary feature if First-Name-DictionaryNote that, similar to the case of overlapping binary features, we don't have to worry about words appearing in the dictionary which are commonly used in another sense. I.e. we can leave dangerouslooking names like quot;Aprilquot; in the first-name dictionary because whenever the first-name feature fires on quot;Aprilquot;, the lexical and date-dictionary features for quot;Aprilquot; will also fire and, assuming that the use of April as quot;datequot; exceeded the use of April as person_start or person_unique, we can expect that the lexical feature will have a high enough a value to outweigh the first-name-dictionary feature. This was confirmed in our test runs no instance of quot;Aprilquot; was tagged as a name, including one case, quot;The death of Ron Brown in April in a similar plane crash ...quot; which could be thought of as somewhat tricky because the month was not followed by a specific date. Note that the system isn't foolproof if a quot;dangerousquot; dictionary word appeared in only one dictionary and did not appear often enough in the training corpus to be included in the vocabulary, but did appear in the test corpus, we would probably mistag it. For NYU's official entry in the MUC-7 evaluation, MENE took in the output of an enhanced version of the more traditional, hand-coded quot;Proteusquot; namedentity tagger which we entered in MUC-6Grishman, 1995. In addition, subsequent to the evaluation, the University of Manitoba Lin, 1998 and IsoQuest, Inc. Krupka and Hausman, 1998 shared with us the outputs of their systems on our training corpora as well as on various test corpora. The output sent to us was the standard MUC-7 output, so our collaborators didn't have to do any special processing for us. These systems were incorporated into MENE as simply three more history views by the following 2 step process The result of all this is that the quot;futuresquot; produced by the three external systems become three quot;external system historiesquot; for MENE. Here is an It is important to note that MENE has features which predict a different future than the future predicted by the external system. This can be seen as the process by which MENE learns the errors which the external system is likely to make. An example of this is that on the evaluation system the feature which predicted person_unique given a tag of person_unique by Proteus had only a 76 higher weight than the feature which predicted person_start given person_unique. In other words, Proteus had a tendency to chop off multi-word names at the first word. MENE learned this and made it easy to override Proteus in this way. In fact, an analysis of the differences between the Proteus output and the MENE Proteus output turned up a significant number of instances in which MENE extended or contracted name boundaries in this way. Given proper training data, MENE can pinpoint and selectively correct the weaknesses of a handcoded system. MENE currently has no direct ability to learn compound features or quot;patternsquot;the quot;historyquot; side of a lexical feature activates based on only a single word, for instance. A sort of pattern-like ability comes into the system from multiple features firing at once. I.e. to predict that quot;Yorkquot; in the name quot;New Yorkquot; is the end of a location, we will have two features firing one predicts location_end when token_i is quot;newquot;. The other predicts location_end when tokeno is quot;yorkquot;. Nevertheless, it is possible that compound features would behave differently from two simultaneously firing quot;atomicquot; features. We integrated this into the model in an ad hoc manner for the external system features, where we constructed features which essentially query the external system history and the section history simultaneously to determine whether they fire. I.e. a particular feature might fire if Proteus predicts person_start, the current section is quot;main body of textquot;, and the future is quot;person_startquot;. This allows MENE to assign a lower a to a Proteus prediction in the preamble vs. a prediction in the main body of text. Proteus, like many hand-coded systems, is more accurate in the main body of the text than in headline-type material. We found that this compound feature gave the system slightly higher performance than we got when we just used section features and external system features separately. It seems reasonable that adding an ability to handle fully general compound features i.e. feature A fires if features B and C both fire would improve system performance based on this limited experiment. In addition to allowing us to predict futures based on multi-word patterns, it would also let us use other promising combinations of features such as distinguishing between capitalization in a headline vs. in the main body of the text. Unfortunately, this experiment will have to wait until we deploy a more sophisticated method of feature selection, as discussed in the next section. Features are chosen by a very simple method. All possible features from the classes we want included in our model are put into a quot;feature poolquot;. For instance, if we want lexical features in our model which activate on a range of token_2 . token2, our vocabulary has a size of V, and we have 29 futures, we will add 5. V 1 29 lexical features to the pool. The V 1 term comes from the fact that we include all words in the vocabulary plus the unknown word. From this pool, we then select all features which fire at least three times on the training corpus. Note that this algorithm is entirely free of human intervention. Once the modeler has selected the classes of features, MENE will both select all the relevant features and train the features to have the proper weightings. We deviate from this basic algorithm in three ways 1. We exclude features which activate on some sort of quot;defaultquot; value of a history view. Many history views have some sort of default value which they display for the vast majority of tokens. For instance, a first-name-dictionary history view would say that the current token is not a name in over 99 of the cases. Rather than adding features which activate both when the token in question is and when it is not a first name, we only include features which activate when the token is a first name. A feature which activated when a token was not a first name, while theoretically not harmful, would have practical disadvantages. First of all, the feature would probably be redundant, because if the frequency of a future given a first-namedictionary hit is constrained by equation 4, then the future frequency given a non-hit is also implicitly constrained. Secondly, since this feature would fire on nearly every token, it would slow down run-time performance. Finally, while maximum entropy models are designed to handle feature overlap, a very high degree of overlap requires more iterations of the maximum entropy estimation routine and can lead to numerical difficulties Ristad, 1998. Like the previous heuristic, this is based on the idea that features predicting named entities are more useful than features predicting the default. Note that this method of feature selection would probably break down if we tried to incorporate general compound features into our model as described in the previous section. The model currently has about 24,000 features when trained on 350 articles of text. If we even considered all pairs of features as potential compound features, the 0n2 compound features which we could build from our atomic features would undoubtedly yield an unacceptable slowdown in the model's performance. Clearly a more sophisticated feature selection routine such as the ones in Berger et al., 1996, or Berger and Printz, 1998 would be required in this case. After having trained the features of an M.E. model and assigned the proper weight a values to each of the features, decoding i.e. quot;marking upquot; a new piece of text is a fairly simple process The Viterbi search is necessary because simply taking the highest-probability future assigned to each token would result in incompatible assignments. For instance, an assignment of person-start, location_end to two consecutive tokens would be invalid. The Viterbi search finds the highest probability path in which there are no two tokens in which the second one cannot follow the first, as defined by a table of all such invalid transitions a similar approach to Sekine et al.. 1998. MENE's maximum entropy training algorithm gives it reasonable performance with moderate-sized training corpora or few information sources, while allowing it to really shine when more training data and information sources are added. Table 2 shows MENE's performance on the MUC-7 quot;dry runquot; corpus, which consisted of 25 articles mostly on the topic of aviation disasters. All systems shown were trained on 350 articles on the same domain this training corpus consisted of about 270,000 words, which our system turned into 321,000 tokens. Note the smooth progression of the scores as more data is added to the system. Also note that, when combined under MENE, the three weakest systems, MENE, Proteus, and Manitoba outperform the strongest single system, IsoQuest's. Finally, the top score of 97.12 from combining all three systems is a very strong result. On a different set of data, the MUC-7 formal run data, the accuracy of the two human taggers who were preparing the answer key was tested and it was discovered that one of them had an F-Measure of 96.95 and the other of 97.60 Marsh and Perzanowski, 1998. Although we don't have human performance measures on the dry run test set, it seems that we have attained a result which is at least competitive with that of a human. We also did a series of runs to examine how the systems performed with different amounts of training data. These experiments are summarized in table 3. Note the 97.38 all-systems result which we achieved by adding 75 articles from the formal-run test corpus to the basic 350-article training data. In addition to being an outstanding performance figure, this number shows MENE's responsiveness to good training material. A few other conclusions can be drawn from this data. First of all, MENE needs at least 20 articles of tagged training data to get acceptable performance on its own. Secondly, there is a minimum amount of training data which is needed for MENE to improve an external system. For Proteus and the Manitoba system, this number seems to be about 80 articles, because they show a degradation of performance at 40. Since the IsoQuest system was stronger to start with, MENE required 150 articles to show an improvement. Note the anomaly in comparing the 250 and 350 article columns. Proteus shows only a very small gain and IsoQuest shows a deterioration. These last 100 articles added to the system were tagged by us at NYU, and we would humbly guess that we tagged them less carefully than the rest of the data which was tagged by BBN and Science Applications International Corporation SAIC. MENE has also been run against all-uppercase data. On this we achieved an F-measure of 88.19 for the MENE-only system and 91.38 for the MENE Proteus system. The latter figure matches the best currently published result Bikel et al., 1997 on within-domain all-caps data. On the other hand, we scored lower on all-caps than BBN's Identifinder in the MUC-7 formal evaluation for reasons which are probably similar to the ones discussed in section 9 in the comparison of our mixed case performances Miller et al., 1998 Borthwick et al., 1998. We have put very little effort into optimizing MENE on this type of corpus and believe that there is room for improvement here. In another experiment, we stripped out all features other than the lexical features and still achieved an F-measure of 88.13. Since these features do not rely on any external knowledge sources and are automatically generated, this result is a strong indicator of MEN E's portability. The MUC-7 formal evaluation involved a shift in topic which was not communicated to the participants beforehand-the training data focused on airline disasters while the test data was on missile and rocket launches. MENE fared much more poorly on this data than it did on the within-domain data quoted above, achieving an F-measure of only 88.80 for the MENE Proteus system and 84.22 for the MENE-only system. While 88.80 was still the fourth highest score out of the twelve participants in the evaluation, we feel that it is necessary to view this number as a cross-domain portability result rather than as an indicator of how the system can do on unseen data within its training domain. Ve believe that if the system had been allowed to train on missilerocket launch articles, its performance on these articles would have been much better. More MENE test results and discussion of the formal run can be found in Borthwick et al., 1998. M.E. has been successfully applied to many other tasks in computational linguistics. Some recent work for which there are solid comparable benchmarks is the work of Adwait Ratnaparkhi at the University of Pennsylvania. He has achieved state-of-the art results by applying M.E. to parsing Ratnaparkhi, 1997a, part-of-speech tagging Ratnaparkhi, 1996, and sentence-boundary detection Reynar and Ratnaparkhi, 1997. Other recent work has applied M.E. to language modeling Rosenfeld, 1994, machine translation Berger et al., 1996, and reference resolution Kehler, 1997. M.E. was first applied to named entity recognition at the MUC-7 conference by Borthwick et al., 1998 and Mikheev and Grover, 1998. Note that part-of-speech tagging is, in many ways, a very similar task to that of named-entity recognition. Ratnaparkhi's tagger is similar to MENE, in that his features look at the surrounding two-word lexical context, but his system makes less use of dictionaries. On the other hand, his system looks at word suffixes and prefixes in the case of unknown words, which is something we haven't tried with MENE and looks at its own output by looking at its previous two tags when making its decision. We do this implicitly through our requirement that the futures we output be consistent, but we found that an attempt to do this more directly by building a consistency feature directly into the model had no effect on our results. At the MUC-7 conference, there were two other interesting systems using statistical techniques from the Language Technology GroupUniversity of Edinborough Mikheev and Grover, 1998 and BBN Miller et al., 1998. Comparisons with the LTG system are difficult since it was a hybrid model in which the text was passed through a five-stage process, only three of which involved maximum entropy and over half of the system's recall came from the two non-statistical phases. The LTG system demonstrated superior performance on the formal run relative to the MENE-Proteus hybrid system 93.39 vs 88.80, but it isn't clear whether their advantage came from superior handcoded rules or superior statistical techniques, because their system is not as easily broken down into separate components as is MENE-Proteus. It is also possible that tighter system integration between the statistical and handcoded components was responsible for some of LTG's relative advantage, but note that MENE-Proteus appears to have an advantage over LTG in terms of portability. We are currently experimenting with porting MENE to Japanese, for instance, and expect that it could be combined with a pre-existing Japanese handcoded system, but it isn't clear that this could be done with the LTG system. Nevertheless, one of our avenues for future research is to look at tighter multi-system integration methods which won't compromise MEN E's essential portability. Table 4 gives a comparison of BBN's HMMbased Identifinder Miller et al., 1998 and NYU's MENE and MENE-Proteus systems on different training and test sets. We are not sure why MENEProteus was hurt more badly by the evaluationtime switch from aviation disaster articles to missuerocket launch articles, but suspect that it may have been due to Identifinder's greater quantity and quality of training data. BBN used 790,000 words of training data to our 321,000. The quality advantage may have come from selecting sentences from a larger corpus for their annotators to tag which were chosen so as to increase the variety of training data. When MENE-only and Identifinder are compared training on the same number of articles and testing on within-domain data, Identifinder still has an edge. We speculate that this is due to the dynamic updating of Identifinder's vocabulary during decoding when person or organization names are recognized, which gives the system a sort of long-distance reference resolution which is lacking in MENE. In addition. BBN's HMM-based system implictly predicts named entities based on consecutive pairs of words rather than based on single words, as is done in MENE, because each type of name has its own bigram language model. In the decoding process, the Viterbi algorithm chooses the sequence of names which yields the highest joint probability of names, words, and features associated with each word. In comparing the maximum entropy and HMMbased approaches to named entity recognition, we are hopeful that M.E. will turn out to be the better method in the end. We think it is possible that some of Identifinder's current advantage can be neutralized by simply adding the just-mentioned features to MENE. On the other hand, we have a harder time seeing how some of MENE's strengths can be integrated into an HMM-based system. It is not clear, for instance, how a wide variety of dictionaries could be added to Identifinder or whether the system could be combined with a handcoded system as was done with our system and the one from LTG. MENE is a very new, and, we feel, still immature system. Work started in October, 1997, and the system described above was not in place until midFebruary. 1998. We believe that we can push the score of the MENE-only system higher by incorporating long-range reference-resolution on MENE's output. We are also missing a large number of acronyms which could be picked up by dynamically building them from entities which MENE had tagged elsewhere and then pulling that data in as a new class of feature. The other key element missing from the current system is a set of general compound features, which, as discussed above, would require the use of a more sophisticated feature selection algorithm. All three of these elements are present in systems such as IsQuest's Kruplca and Hausman, 1998, and their absence from MENE probably explains much of the reason why the MENE-only system failed to perform at the state-of-the-art. We intend to add all of these elements to MENE in the near future to test this hypothesis. Nevertheless, we believe that we have already demonstrated some very useful results. MENE is highly portable, as we have already demonstrated with our result on upper-case English text and even in its current state, its results are already comparable to that of the only other purely statistical English NE system which we are aware of Miller et al., 1998. As shown with our result on running MENE with only the lexical features that it learns from the training corpus, porting MENE can be done with very little effort if appropriate training data is providedit isn't even necessary to provide it with dictionaries to generate an acceptable result. We are working on a port to Japanese NE to further demonstrate MENE's flexibility. However, we believe that the results on combining MENE with other systems are some of the most intriguing. We would hypothesize that, given sufficient training data, any handcoded system would benefit from having its output passed to MENE as a final step. MENE also opens up new avenues for collaboration whereby different organizations could focus on different aspects of the problem of N.E. recognition with the maximum entropy system acting as an arbitrator. MENE also offers the prospect of achieving very high performance with very little effort. Since MENE starts out with a fairly high base score just on its own, we speculate that a MENE user could then construct a hand-coded system which only focused on MENE's weaknesses, while skipping the areas in which MENE is already strong. Finally, one can imagine a user acquiring licenses to several different N.E. systems, generating some training data, and then combining it all under a MENE-like system. We have shown that this approach can yield performance which is competitive with that of a human tagger. We would like to thank Troy Straszheim for writing the Viterbi search routine used in this work.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 608,\n        \"samples\": [\n          \"Title Coarse-To-Fine N-Best Parsing And MaxEnt Discriminative Reranking Abstract Discriminative reranking is one method for constructing high-performance statistical parsers Collins, 2000.A discriminative reranker requires a source of candidate parses for each sentence.This paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse-to-fine generative parser Charniak, 2000.This method generates 50-best lists that are of substantially higher quality than previously obtainable.We used these parses as the input to a MaxEnt reranker Johnson et al., 1999; Riezler et al., 2002 that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91.0 on sentences of length 100 or less. Conclusion This paper has described a dynamic programming n-best parsing algorithm that utilizes a heuristic coarse-to-fine refinement of parses.Because the coarse-to-fine approach prunes the set of possible parse edges beforehand, a simple approach which enumerates the n-best analyses of each parse edge is not only practical but quite efficient.We use the 50-best parses produced by this algorithm as input to a MaxEnt discriminative reranker.The reranker selects the best parse from this set of parses using a wide variety of features.The system we described here has an f-score of 0.91 when trained and tested using the standard PARSEVAL framework.This result is only slightly higher than the highest reported result for this test-set, Bods .907 Bod, 2003.More to the point, however, is that the system we describe is reasonably efficient so it can be used for the kind of routine parsing currently being handled by the Charniak or Collins parsers.A 91.0 f-score represents a 13 reduction in fmeasure error over the best of these parsers.2 Both the 50-best parser, and the reranking parser can be found at ftpftp.cs.brown.edupubnlparser, named parser and reranker respectively.Acknowledgements We would like to thanks Michael Collins for the use of his data and many helpful comments, and Liang Huang for providing an early draft of his paper and very useful comments on our paper.Finally thanks to the National Science Foundation for its support NSF IIS-0112432, NSF 9721276, and NSF DMS-0074276.\",\n          \"Title Scaling Web-Based Acquisition Of Entailment Relations Abstract Paraphrase recognition is a critical step for natural language interpretation.Accordingly, many NLP applications would benefit from high coverage knowledge bases of paraphrases.However, the scalability of state-of-the-art paraphrase acquisition approaches is still limited.We present a fully unsupervised learning algorithm for Web-based extraction an extended model of paraphrases.We focus on increased scalability and generality with respect to prior work, eventually aiming at a full scale knowledge base.Our current implementation of the algorithm takes as its input a verb lexicon and for each verb searches the Web for related syntactic entailment templates.Experiments show promising results with respect to the ultimate goal, achieving much better scalability than prior Web-based methods. Conclusion We have described a scalable Web-based approach for entailment relation acquisition which requires only a standard phrasal lexicon as input.This minimal level of input is much simpler than required by earlier web-based approaches, while succeeding to maintain good performance.This result shows that it is possible to identify useful anchor sets in a fully unsupervised manner.The acquired templates demonstrate a broad range of semantic relations varying from synonymy to more complicated entailment.These templates go beyond trivial paraphrases, demonstrating the generality and viability of the presented approach.From our current experiments we can expect to learn about 5 relations per lexicon entry, at least for the more frequent entries.Moreover, looking at the extended test, we can extrapolate a notably larger yield by broadening the search space.Together with the fact that we expect to find entailment relations for about 85 of a lexicon, it is a significant step towards scalability, indicating that we will be able to extract a large scale KB for a large scale lexicon.In future work we aim to improve the yield by increasing the size of the sample-corpus in a qualitative way, as well as precision, using statistical methods such as supervised learning for better anchor set identification and cross-correlation between different pivots.We also plan to support noun phrases as input, in addition to verb phrases.Finally, we would like to extend the learning task to discover the correct entailment direction between acquired templates, completing the knowledge required by practical applications.Like Lin and Pantel, 2001, learning the context for which entailment relations are valid is beyond the scope of this paper.As stated, we learn entailment relations holding for some, but not necessarily all, contexts.In future work we also plan to find the valid contexts for entailment relations.\",\n          \"Title Exploiting Diverse Knowledge Sources Via Maximum Entropy In Named Entity Recognition Abstract This paper describes a novel statistical namedentity i.e.quot;proper namequot; recognition system built around a maximum entity framework.By working within the framework of maximum entropy theory and utilizing a flexible object-based architecture, the system is able to make use of an extraordinarily diverse range of knowledge sources in making its tagging decisions.These knowledge sources include capitalization features, lexical features, features indicating the current section of text i.e. headline or main body, and dictionaries of single or multi-word terms.The purely statistical system contains no hand-generated patterns and achieves a result comparable with the best statistical systems.However, when combined with other handcoded systems, the system achieves scores that exceed the highest comparable scores thus-far published. Conclusion MENE's maximum entropy training algorithm gives it reasonable performance with moderate-sized training corpora or few information sources, while allowing it to really shine when more training data and information sources are added.Table 2 shows MENE's performance on the MUC-7 quot;dry runquot; corpus, which consisted of 25 articles mostly on the topic of aviation disasters.All systems shown were trained on 350 articles on the same domain this training corpus consisted of about 270,000 words, which our system turned into 321,000 tokens.Note the smooth progression of the scores as more data is added to the system.Also note that, when combined under MENE, the three weakest systems, MENE, Proteus, and Manitoba outperform the strongest single system, IsoQuest's.Finally, the top score of 97.12 from combining all three systems is a very strong result.On a different set of data, the MUC-7 formal run data, the accuracy of the two human taggers who were preparing the answer key was tested and it was discovered that one of them had an F-Measure of 96.95 and the other of 97.60 Marsh and Perzanowski, 1998.Although we don't have human performance measures on the dry run test set, it seems that we have attained a result which is at least competitive with that of a human.We also did a series of runs to examine how the systems performed with different amounts of training data.These experiments are summarized in table 3.Note the 97.38 all-systems result which we achieved by adding 75 articles from the formal-run test corpus to the basic 350-article training data.In addition to being an outstanding performance figure, this number shows MENE's responsiveness to good training material.A few other conclusions can be drawn from this data.First of all, MENE needs at least 20 articles of tagged training data to get acceptable performance on its own.Secondly, there is a minimum amount of training data which is needed for MENE to improve an external system.For Proteus and the Manitoba system, this number seems to be about 80 articles, because they show a degradation of performance at 40.Since the IsoQuest system was stronger to start with, MENE required 150 articles to show an improvement.Note the anomaly in comparing the 250 and 350 article columns.Proteus shows only a very small gain and IsoQuest shows a deterioration.These last 100 articles added to the system were tagged by us at NYU, and we would humbly guess that we tagged them less carefully than the rest of the data which was tagged by BBN and Science Applications International Corporation SAIC.MENE has also been run against all-uppercase data.On this we achieved an F-measure of 88.19 for the MENE-only system and 91.38 for the MENE Proteus system.The latter figure matches the best currently published result Bikel et al., 1997 on within-domain all-caps data.On the other hand, we scored lower on all-caps than BBN's Identifinder in the MUC-7 formal evaluation for reasons which are probably similar to the ones discussed in section 9 in the comparison of our mixed case performances Miller et al., 1998 Borthwick et al., 1998.We have put very little effort into optimizing MENE on this type of corpus and believe that there is room for improvement here.In another experiment, we stripped out all features other than the lexical features and still achieved an F-measure of 88.13.Since these features do not rely on any external knowledge sources and are automatically generated, this result is a strong indicator of MEN E's portability.The MUC-7 formal evaluation involved a shift in topic which was not communicated to the participants beforehand-the training data focused on airline disasters while the test data was on missile and rocket launches.MENE fared much more poorly on this data than it did on the within-domain data quoted above, achieving an F-measure of only 88.80 for the MENE Proteus system and 84.22 for the MENE-only system.While 88.80 was still the fourth highest score out of the twelve participants in the evaluation, we feel that it is necessary to view this number as a cross-domain portability result rather than as an indicator of how the system can do on unseen data within its training domain.Ve believe that if the system had been allowed to train on missilerocket launch articles, its performance on these articles would have been much better.More MENE test results and discussion of the formal run can be found in Borthwick et al., 1998.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["df.to_csv(\"dataset/final_cleaned.csv\", index=False)"],"metadata":{"id":"g2EMBqlFMsoS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**create train test splits**"],"metadata":{"id":"1qZINS3PbCaa"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n","print(f\"Total set size: {len(df)}\")\n","print(f\"Training set size: {len(train_df)}\")\n","print(f\"Testing set size: {len(test_df)}\")\n","\n","#save\n","train_df.to_csv(\"dataset/train.csv\", index=False)\n","test_df.to_csv(\"dataset/test.csv\", index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hv6rwj4Mbcmi","executionInfo":{"status":"ok","timestamp":1731070750829,"user_tz":-330,"elapsed":847,"user":{"displayName":"21dcs022 BHARGAV","userId":"14054464433858683476"}},"outputId":"fb307fe5-5694-4c5f-f6dc-6777fa9030cd","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total set size: 608\n","Training set size: 547\n","Testing set size: 61\n"]}]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","\n","#**start point**\n","\n","\n"],"metadata":{"id":"4M_bBqAeT2u7"}},{"cell_type":"markdown","source":["\n","\n","---\n","**DATA LOADER**\n","\n","---\n"],"metadata":{"id":"VDgRzucH1vyv"}},{"cell_type":"code","source":["!pip install transformers datasets torch rouge-score\n"],"metadata":{"collapsed":true,"id":"Rp71snJl3TJX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734014687821,"user_tz":-330,"elapsed":5945,"user":{"displayName":"21dcs022 BHARGAV","userId":"14054464433858683476"}},"outputId":"edded148-c86b-4411-825d-c4984a77d37c"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n","Requirement already satisfied: rouge-score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.9.1)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.4.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n"]}]},{"cell_type":"markdown","source":["\n","\n","---\n","multinews dataset for later\n"],"metadata":{"id":"mukSOBNJHMNg"}},{"cell_type":"code","source":["# from datasets import load_dataset\n","# dataset=load_dataset('multi_news',split='test')"],"metadata":{"id":"FSRnPcDtHAUX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"X6K-NYh-HSki"}},{"cell_type":"code","source":["import pandas as pd\n","from datasets import Dataset\n","\n","df = pd.read_csv(\"dataset/final_cleaned.csv\")\n","\n","dataset = Dataset.from_pandas(df)\n","dataset\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XQEn_1Pu3H_Q","executionInfo":{"status":"ok","timestamp":1734014689071,"user_tz":-330,"elapsed":1253,"user":{"displayName":"21dcs022 BHARGAV","userId":"14054464433858683476"}},"outputId":"ba22041e-09bf-491e-b3f4-7aeb2198b38a"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['Index', 'Article', 'Summary'],\n","    num_rows: 608\n","})"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["scisumm = dataset.train_test_split(test_size=0.1, seed =42 )\n","scisumm\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lhExc5JXEXte","executionInfo":{"status":"ok","timestamp":1734014690112,"user_tz":-330,"elapsed":3,"user":{"displayName":"21dcs022 BHARGAV","userId":"14054464433858683476"}},"outputId":"2539000a-24e1-4109-e2f8-0bc169db3564"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['Index', 'Article', 'Summary'],\n","        num_rows: 547\n","    })\n","    test: Dataset({\n","        features: ['Index', 'Article', 'Summary'],\n","        num_rows: 61\n","    })\n","})"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["from transformers import T5Tokenizer\n","\n","tokenizer = T5Tokenizer.from_pretrained('t5-small')"],"metadata":{"collapsed":true,"id":"jSfXC4ecE1Mr","executionInfo":{"status":"ok","timestamp":1734014694710,"user_tz":-330,"elapsed":1989,"user":{"displayName":"21dcs022 BHARGAV","userId":"14054464433858683476"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def preprocess_function(examples):\n","    inputs = [doc for doc in examples[\"Article\"]]\n","    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n","\n","    labels = tokenizer(examples[\"Summary\"], max_length=128, truncation=True, padding=True)\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs"],"metadata":{"id":"Wh_ehNLIGBAw","executionInfo":{"status":"ok","timestamp":1734014699915,"user_tz":-330,"elapsed":393,"user":{"displayName":"21dcs022 BHARGAV","userId":"14054464433858683476"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["tokenized_scisumm=scisumm.map(preprocess_function, batched=True)\n","tokenized_scisumm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":255,"referenced_widgets":["25f193345e0e4a848e5abdadc76a2bb0","3292da00d2544e30bc76191d2044143d","80a8290340df408190903f34a6900cd8","6aa16514afc24c0c8e297c39c74badc3","e776470bfe4d4390a63c70bde4f856eb","8bfbfe445482450988bfe90c0fa4a99e","a4dcc2d5a2b64df3bdfa5277355c401f","5664221f05e34cdea7be87ee4f3445da","17dc42100e44406d8c43407cc84629f1","e71d9fb5a2db4bea93746f79854094e7","afadf4955ac549a7b21d493c7e39d298","62075485a028477897487c8597a7d453","e73bf9ac74b04550b8a032df61517f7a","e352faa97278402da66b78a97d627000","8afc9b91aa92420eb390b52357086f44","bd073c3c119c4fc29222ba93b233c1d1","3674941bd75146e984ceebfa039bdc2c","553ed739e3f84230b54979ad4623d34e","6eefd41c1bab411290c074e6b01a8a5b","4a22c6663f70472b8f6f970b5c239be0","fe9bd13cd8c04a399f598f84b9c25d6a","b3722d36a9eb44b1a9fdb3a3a8851997"]},"id":"IWxPyvzHGf7_","executionInfo":{"status":"ok","timestamp":1734014751612,"user_tz":-330,"elapsed":48200,"user":{"displayName":"21dcs022 BHARGAV","userId":"14054464433858683476"}},"outputId":"48c4177b-d15b-4968-c349-bd7dea610ffe"},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/547 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25f193345e0e4a848e5abdadc76a2bb0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/61 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62075485a028477897487c8597a7d453"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['Index', 'Article', 'Summary', 'input_ids', 'attention_mask', 'labels'],\n","        num_rows: 547\n","    })\n","    test: Dataset({\n","        features: ['Index', 'Article', 'Summary', 'input_ids', 'attention_mask', 'labels'],\n","        num_rows: 61\n","    })\n","})"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["tokenized_scisumm = tokenized_scisumm.remove_columns(['Index', 'Article', 'Summary'])\n","tokenized_scisumm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"waYI5loSPOk6","executionInfo":{"status":"ok","timestamp":1734014903835,"user_tz":-330,"elapsed":400,"user":{"displayName":"21dcs022 BHARGAV","userId":"14054464433858683476"}},"outputId":"3313d2e4-9e65-405a-e4a4-bea1d02da8f5"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['input_ids', 'attention_mask', 'labels'],\n","        num_rows: 547\n","    })\n","    test: Dataset({\n","        features: ['input_ids', 'attention_mask', 'labels'],\n","        num_rows: 61\n","    })\n","})"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["from transformers import T5ForConditionalGeneration, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","\n","data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=\"t5-small\")\n","\n","model=T5ForConditionalGeneration.from_pretrained('t5-small')\n","model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["96ee79a0937b4d0aa3d84896b8172bed","8a6f2a5a37e64a0e8eb2a34bacbc3293","b2d53a897e1449f0bdbd5e56f9a1cabd","312d48abcd394031a27fcd97a1ea1330","21d06e80b12f44dcaf01b0718bab6d5d","b92ae0f2202c47cf82257a1bc90a47d5","d3b4b4c9363242dd844c389f1229fe8e","56a323e5564040e981830a49abb1e4d0","b10c7a131c63462492c8a1fe4c03e613","3b918f5eb98d49efa0a4db03ea15e618","790997be81de425489494bd8e2a1762e","8a1c76ad55ee496092c45c653799da05","c38849b2711c46c4bb795b775a2fdc60","335c8c0601da458d817a188e6173e72a","e90ec56afe15445ba47890c2251072bb","500829b8c4b0457f867fc1906f75ed2d","b21194b5df2441d790692cb5b6d035bf","21f4148afc3c4cd3becc4a76230aabc3","ca6f92bb45584608994ab0da1eb3fd35","111cea8adf614704b15d705e0c211a18","e4c8e79853424b4fb77d299f386402d2","f5bd613cb81640438bc67cf8211ff964","f1f6ef7772ff493b82d46dc2aafcf567","9ecf48f9ca61460e871d686ec4823aa9","38b0eeb5c76541ae964ac8c0195411fd","ae7e8687564540f6acf3ed244925b6a1","6d53231a720c4cf89d106b4a281493ad","7bc801117b8b4e8da69600e4c191d4cc","fbe1264d226a4b6a87f302b85f590ec5","c7ab5a0a3bb04687ba7086e9ed50f0c3","06e4316de22f4d61aa94ba8c547ec58a","5982cbd7a5274aa782d938e4ac9415b3","aa3d9a8f759a42058b6e592cd259a87b"]},"id":"mrdBhUMKH1U1","executionInfo":{"status":"ok","timestamp":1734014930977,"user_tz":-330,"elapsed":18879,"user":{"displayName":"21dcs022 BHARGAV","userId":"14054464433858683476"}},"outputId":"bcf8c95c-4a4a-42ef-c47e-b473e09f4e64","collapsed":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96ee79a0937b4d0aa3d84896b8172bed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a1c76ad55ee496092c45c653799da05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1f6ef7772ff493b82d46dc2aafcf567"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["T5ForConditionalGeneration(\n","  (shared): Embedding(32128, 512)\n","  (encoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 512)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=512, bias=False)\n","              (k): Linear(in_features=512, out_features=512, bias=False)\n","              (v): Linear(in_features=512, out_features=512, bias=False)\n","              (o): Linear(in_features=512, out_features=512, bias=False)\n","              (relative_attention_bias): Embedding(32, 8)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseActDense(\n","              (wi): Linear(in_features=512, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): ReLU()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1-5): 5 x T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=512, bias=False)\n","              (k): Linear(in_features=512, out_features=512, bias=False)\n","              (v): Linear(in_features=512, out_features=512, bias=False)\n","              (o): Linear(in_features=512, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseActDense(\n","              (wi): Linear(in_features=512, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): ReLU()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (decoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 512)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=512, bias=False)\n","              (k): Linear(in_features=512, out_features=512, bias=False)\n","              (v): Linear(in_features=512, out_features=512, bias=False)\n","              (o): Linear(in_features=512, out_features=512, bias=False)\n","              (relative_attention_bias): Embedding(32, 8)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=512, bias=False)\n","              (k): Linear(in_features=512, out_features=512, bias=False)\n","              (v): Linear(in_features=512, out_features=512, bias=False)\n","              (o): Linear(in_features=512, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseActDense(\n","              (wi): Linear(in_features=512, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): ReLU()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1-5): 5 x T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=512, bias=False)\n","              (k): Linear(in_features=512, out_features=512, bias=False)\n","              (v): Linear(in_features=512, out_features=512, bias=False)\n","              (o): Linear(in_features=512, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=512, bias=False)\n","              (k): Linear(in_features=512, out_features=512, bias=False)\n","              (v): Linear(in_features=512, out_features=512, bias=False)\n","              (o): Linear(in_features=512, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseActDense(\n","              (wi): Linear(in_features=512, out_features=2048, bias=False)\n","              (wo): Linear(in_features=2048, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): ReLU()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",")"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["# training arguments\n","training_arguments = Seq2SeqTrainingArguments(\n","            output_dir='./results',\n","            evaluation_strategy='epoch',\n","\n","            save_strategy='epoch',  #save model state after each epoch (space consuming)\n","            #save_strategy='no',  #1. use to save model only once after finishing training (comment out above line in case)\n","\n","            logging_dir='./logs',\n","            learning_rate=2e-5,\n","            per_device_train_batch_size=2,\n","            per_device_eval_batch_size=2,\n","            weight_decay=0.01,\n","            save_total_limit=4,\n","            num_train_epochs=2,\n","            # remove_unused_columns=False,\n","            fp16=True,\n","            )\n","\n","\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_arguments,\n","    train_dataset=tokenized_scisumm['train'],\n","    eval_dataset=tokenized_scisumm['test'],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator\n","    )\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JxVnMnsKImZZ","executionInfo":{"status":"ok","timestamp":1734015265297,"user_tz":-330,"elapsed":3503,"user":{"displayName":"21dcs022 BHARGAV","userId":"14054464433858683476"}},"outputId":"17264fb5-7ccc-4bb2-f539-358ab6979700"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","<ipython-input-16-ce7ebb9c9a62>:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n","  trainer = Seq2SeqTrainer(\n"]}]},{"cell_type":"code","source":["trainer.train()\n","\n","#trainer.save_model()  # 2.save model after completing training (use if (1.) is followed)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":418},"id":"jW2FmzPZLGus","outputId":"61924932-bfbe-42f0-f112-de3299d778ae","executionInfo":{"status":"ok","timestamp":1734021893218,"user_tz":-330,"elapsed":6622483,"user":{"displayName":"21dcs022 BHARGAV","userId":"14054464433858683476"}}},"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":[" ··········\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.18.7"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/drive/MyDrive/Colab Notebooks/MDS/wandb/run-20241212_145806-q7ecg7vp</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/bhar_gav-national-institute-of-technology-hamirpur/huggingface/runs/q7ecg7vp' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/bhar_gav-national-institute-of-technology-hamirpur/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/bhar_gav-national-institute-of-technology-hamirpur/huggingface' target=\"_blank\">https://wandb.ai/bhar_gav-national-institute-of-technology-hamirpur/huggingface</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/bhar_gav-national-institute-of-technology-hamirpur/huggingface/runs/q7ecg7vp' target=\"_blank\">https://wandb.ai/bhar_gav-national-institute-of-technology-hamirpur/huggingface/runs/q7ecg7vp</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='548' max='548' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [548/548 1:46:28, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.483852</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>1.048600</td>\n","      <td>0.399393</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=548, training_loss=1.0214617304558302, metrics={'train_runtime': 6621.9363, 'train_samples_per_second': 0.165, 'train_steps_per_second': 0.083, 'total_flos': 296127861620736.0, 'train_loss': 1.0214617304558302, 'epoch': 2.0})"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["**Evaluate**"],"metadata":{"id":"n5a6OEB13lbW"}},{"cell_type":"code","source":["from transformers import DataCollatorForSeq2Seq\n","from torch.utils.data import DataLoader\n","import torch\n","\n","data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n","\n","def generate_summaries(test_dataset, model, tokenizer, batch_size=4, max_output_length=150):\n","    model.eval()\n","    all_summaries = []\n","\n","    dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=data_collator)\n","\n","    for batch in dataloader:\n","        input_ids = batch['input_ids'].to(model.device)\n","        attention_mask = batch['attention_mask'].to(model.device)\n","\n","        summary_ids = model.generate(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            max_length=max_output_length,\n","            num_beams=4,\n","            length_penalty=2.0,\n","            early_stopping=True\n","        )\n","\n","\n","        summaries = tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n","        all_summaries.extend(summaries)\n","\n","    return all_summaries\n","\n","generated_summaries = generate_summaries(tokenized_scisumm['test'], model, tokenizer)\n","\n","for i in range(5):\n","    print(f\"Generated Summary {i+1}: {generated_summaries[i]}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GNtgf1xRPjdl","executionInfo":{"status":"ok","timestamp":1734023043483,"user_tz":-330,"elapsed":573068,"user":{"displayName":"21dcs022 BHARGAV","userId":"14054464433858683476"}},"outputId":"773a48de-901e-4428-8c5f-66a81e277dc8"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Generated Summary 1: Discriminative Training And Maximum Entropy Models For Statistical Machine Translation Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract Abstract\n","Generated Summary 2: This paper presents a method for identifying an opinion with its holder and topic, given a sentence from online news media texts. We introduce an approach of exploiting the semantic structure of a sentence, anchored to an opinion bearing verb or adjective. This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using data from FrameNet. We decompose our task into three phases identifying an opinion-bearing word, labeling semantic roles related to the word in the sentence, and then finding the.\n","Generated Summary 3: We investigate automatic classification of speculative language hedging, in biomedical text using weakly supervised machine learning. Our contributions include a precise description of the task with annotation guidelines, analysis and discussion, a probabilistic weakly supervised learning model, and experimental evaluation of the methods presented.We show that hedge classification is feasible using weakly supervised ML, and point toward avenues for future research.The automatic processing of scientific papers using NLP and machine learning ML techniques is an increasingly important aspect of technical informatics.\n","Generated Summary 4: We formulate the problem of nonprojective dependency parsing as a polynomial-sized integer linear program. Our formulation is able to handle non-local output features in an efficient manner; not only is it compatible with prior knowledge encoded as hard constraints, it can also learn soft constraints from data. In particular, our model is able to learn correlations among neighboring arcs siblings and grandparents, word valency, and tendencies toward nearlyprojective parses. The model parameters are learned in a max-margin framework by employing a linear programming relaxation. We formulate the problem of nonprojective dependency parsing as a polynomial-sized integer linear program\n","Generated Summary 5: In this paper, we present a WSD algorithm that relies on a different intuition 2 Two different words are likely to have similar meanings if they occur in identical local contexts. We present an algorithm that uses the same knowledge sources to disambiguate different words. The algorithm does not require a sense-tagged corpus and exploits the fact that two different words are likely to have similar meanings if they occur in identical local contexts. Separate classifiers have to be trained for different words.We present an algorithm that uses the same knowledge sources to disambiguate a word with a different intuition 2 Two occurrences of the same word are likely to have similar meanings if they\n"]}]},{"cell_type":"code","source":["!pip install evaluate\n"],"metadata":{"id":"-Q8AI-1KgTUm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734023126919,"user_tz":-330,"elapsed":3779,"user":{"displayName":"21dcs022 BHARGAV","userId":"14054464433858683476"}},"outputId":"8d1dbb1f-526a-48e4-bae8-9994062ea481"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting evaluate\n","  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n","Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.6)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n","Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.26.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.9)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n","Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: evaluate\n","Successfully installed evaluate-0.4.3\n"]}]},{"cell_type":"code","source":["import evaluate\n","\n","rouge = evaluate.load(\"rouge\")\n","\n","def decode_labels(labels, tokenizer):\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","    return decoded_labels\n","\n","generated_summaries = generate_summaries(tokenized_scisumm['test'], model, tokenizer)\n","reference_summaries = decode_labels(tokenized_scisumm['test']['labels'], tokenizer)\n","\n","\n","def calculate_rouge(generated_summaries, reference_summaries):\n","    results = rouge.compute(predictions=generated_summaries, references=reference_summaries)\n","    return results\n","\n","rouge_results = calculate_rouge(generated_summaries, reference_summaries)\n","\n","\n","print(\"ROUGE Scores:\")\n","print(rouge_results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":84,"referenced_widgets":["1d0809a4c2f445b8ada2a1dffaaf616f","b2073f0d379449f3aa24e6b59faf8db8","5007ff5acade4ec09e1650de08aa593a","52ceb771a418431797b002ff6b295024","8c8d08ee4649458aac8d7cb3d6230582","3d3bddc80229419dacb67437f25ff956","fc9182a1c8464209b6de8c71e158dd8c","0cc57923aecc452eb3fa1de9439fa735","23c35b826a2640b3af04f6fcc5885383","124027340c4d4fd2bc843f5b957948d2","edd506273199497881a9c8b245a71d08"]},"id":"pg2EY006dndF","executionInfo":{"status":"ok","timestamp":1734023715714,"user_tz":-330,"elapsed":585408,"user":{"displayName":"21dcs022 BHARGAV","userId":"14054464433858683476"}},"outputId":"d583b6f4-845e-4b57-bce9-1174849d5ffb"},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d0809a4c2f445b8ada2a1dffaaf616f"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["ROUGE Scores:\n","{'rouge1': 0.6950126022757905, 'rouge2': 0.6364337606453072, 'rougeL': 0.6408800051875528, 'rougeLsum': 0.6419400906456656}\n"]}]},{"cell_type":"markdown","source":["\n","\n","---\n","**su4 included**\n"],"metadata":{"id":"qn0GC3FmmHwi"}},{"cell_type":"code","source":["import evaluate\n","\n","rouge = evaluate.load(\"rouge\")\n","\n","def decode_labels(labels, tokenizer):\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","    return decoded_labels\n","\n","generated_summaries = generate_summaries(tokenized_scisumm['test'], model, tokenizer)\n","reference_summaries = decode_labels(tokenized_scisumm['test']['labels'], tokenizer)\n","\n","\n","def calculate_rouge(generated_summaries, reference_summaries):\n","    results = rouge.compute(predictions=generated_summaries, references=reference_summaries, rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\", \"rouge4\"])\n","    return results\n","\n","rouge_results = calculate_rouge(generated_summaries, reference_summaries)\n","\n","\n","print(\"ROUGE Scores:\")\n","print(rouge_results)"],"metadata":{"id":"D35aO9QnrbzT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","Bert Model  : https://huggingface.co/google-t5/t5-small\n","---\n","\n"],"metadata":{"id":"cwSaA6Vs7QPz"}},{"cell_type":"markdown","source":["END\n","\n","---\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"cQQRJjoDSrIQ"}},{"cell_type":"markdown","source":["\n","\n","---\n","**live test**\n"],"metadata":{"id":"TJm3DclpVBbF"}}]}